%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass{article}  % Comment this line out if you need a4paper
\usepackage{ijcai16}
\usepackage{times}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{subcaption}
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Rapidly Exploring Learning Trees
}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{\arg\!\min} 
\DeclareMathOperator*{\argmax}{\arg\!\max} 
\author{Kyriacos Shiarlis \\
University of Amsterdam \\
k.c.shiarlis@uva.nl
\And 
Joao Messias \\
University of Amsterdam \\ 
jmessias@uva.nl 
\And
Shimon Whiteson \\
University of Oxford \\
shimon.whiteson@cs.ox.ac.uk 
}
\newcommand{\jm}[1]{\textcolor{blue}{Joao: #1}}

\newcommand{\sw}[1]{\textcolor{red}{SW: #1}}
\newcommand{\ks}[1]{\textcolor{green}{SW: #1}}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Learning from demonstration (LfD) \cite{argall2009survey} is of great interest to roboticists because it can avoid the need for tedious manual programming of complex behaviours. While most LfD methods rely on supervised learning to directly learn policies, certain approaches, namely inverse optimal control (IOC) \cite{kalman1964linear} and inverse reinforcement learning (IRL) \cite{abbeel2004apprenticeship} instead learn \emph{cost functions} from demonstration. These cost functions are then used to \emph{plan} the robot's behaviour. 

Cost functions tend to be more general and robust to changes in the environment. For example, if the friction in a manipulator's joints changes. A robot trained in a supervised manner has less chance of adapting than one that has been trained using IRL or IOC because the cost function remains unchanged. In addition, cost functions are thought to be more succint representations of the aims of the agent \cite{abbeel2004apprenticeship}. %For example, an agent that whose aim is to get as fast as possible to a goal location. The policy for such an agent will be much harder to interpret when compared to its cost function. 

However, inverse methods also have practical limitations. IRL for example requires modeling the task as a Markov decision process (MDP), which is typically impractical in robotics. for several reasons  Firstly, an MDP assumes full observability. Secondly, planning in an MDP, which IRL methods must do repeatedly, is costly, especially in continuous and high-dimensional domains. Finally, the lack of scalability and the assumption of full observability prohibit the natural modeling of important factors such as kinodynamic constraints. 

For these reasons, several researchers have proposed replacing the planning step in IRL with more convienient planners, e.g., Maximum Margin Planning (MMP) \cite{ratliff2006maximum} uses $A^*$ search for planning. This avoids the need to do all the planning in advance, as is typical when solving an MDP. It also avoids the need to model all aspects of the environment since the robot can quickly replan its trajectory to account for uncertainty. However, deterministic search methods such as A$^*$ require discretisation of the state space, and quickly run into scalability problems as the discretisation becomes finer. Furthermore, this discretisation again ignores kinodynamic constraints.

In path planning, Rapidly Exploring Random Trees (RRTs) \cite{lavalle1998rapidly} are popular because they cope well with continuous an high-dimensional domains and can elegantly handle kinodynamic constraints. The RRT$^*$ algorithm \cite{karaman2011sampling}, which extends RRTs to incorporate a cost function, is especially useful. However, the cost functions used by RRT$^*$ are typically simple and hand-coded.  Furthermore, to our knowledge, no methods have been developed to learn RRT$^*$ cost functions from demonstrations.

In this paper, we present methods for inverse reinforcement learning of RRT$^*$ cost functions. Our methods require no additional planner assumptions other than those inherent in RRT$^*$, making them particularly easy to implement. We begin by showing that the sample-based nature of RRT$^*$ requires a novel formulation of the learning problem. We then formulate algorithms that allow these cost functions to be learned effectively. Finally, we evaluate our methods on real and simulated data from a social navigation scenario. 

\section{Related Work}

\sw{General IRL related work here.}

Substantial research has applied IRL to robotics \cite{henry2010learning},\cite{abbeel2008apprenticeship}, \cite{vasquez2014inverse}. Because of the need to model the environment as an MDP, researchers usually discretise the state-action space. This introduces several key limitations. First, the size of the state-action space encountered in robotics is often prohibitive for such models. Second, the specific kinodynamic constraints of the robot cannot be easily be expressed by such representations, as they introduce non-Markovian dynamics. \ks{Depending on the state-action space discretisation, past actions can infuence how easy it is to move to any a subsequent state. However the transition function only considers only one action in the past, and it is therefore an expectation over possible past configurations. As a result it tends to give the wrong probability distribution over next states for the actual physical system. Do I have to explain this in detail? Maybe Joao has a citation from his research?}

To apply IRL to more realistic situations, researchers usually try to replace the MDP model while retaining the main idea behind IRL, i.e., learning the underlying cost function of a planner using data from demonstrations. 
 In \cite{ziebart2010modelingthesis} for example the Maximum Entropy IRL method was shown to work in domains of linear continouous dynamics with the optimal controller being a Linear-Quadratic Regulator. Since linear dynamics are hard to come by in robotics, in \cite{2012-cioc} locally linear aproximations are considered. These bring about locally consistent rewards, and achieve good performance in a range of tasks that were previously too hard for MDPs. However the optimisation of the cost function using such planners can never be guaranteed to be optimal. As our method is based on an RRT$^*$ planner it comes with the guarantee of asymptotic optimality.

Other approaches to the problem use hybrid planners. Inverse Optimal Heuristic Control (IOHC) \cite{ratliff2009inverse} models the long term goals of the agent as a coarse state MDP, to ensure tractability, while using supervised learning to determine the local policy of the agent at each state. Graph-Based IOC \cite{byravan2015graph} uses discrete optimal control on a coarse graph and the actual path is executed using local trajectory optimisation techinques such as CHOMP \cite{ratliff2009chomp}, which would otherwise suffer from local minima. The method is shown to be effective in robotic manipulation where the number of degrees of freedom would otherwise pose problems for MDPs. While effective, these methods employ complex and domain specific planning formulations whose implementation might be undesirable for other robotics tasks. The method presented here employs widely used planners, making them versatile and easy to implement. 

Another more recent graph-based concept is that of adaptive state graphs used in \cite{okallearning}. In this work the authors build a controller graph before doing any learning. This controller graph is akin to \emph{options} in semi-Markov decision processes, and allows for a more flexible representation of the state-action space. However, the controller used to learn underlying cost function is not the same as the one used to execute the robot's behaviour. 

Our work, can also be viewed as a graph, or rather tree-based approach. Instead of building a controller graph first and then using different controllers to optimise trajectories, we build a controller \emph{tree} on the fly.


\section{Background}
We begin with background on path planning and inverse reinforcement learning for path planning.

\subsection{Path Planning \label{subsec:path_planning}}
Path planning occurs in a space $\mathcal{S}$ of possible configurations of the robot. A configuration $s \in \mathcal{S}$ is usually continuous, and often represents spatial quantities such as position and orientation. A path planner seeks an obstacle-free path $\zeta_{o,g} = (s_1,s_2,s_3$ $\ldots,s_{l_{\zeta}}) $ of length $l_{\zeta}$, from an initial configuration $o = s_1$ to a goal configuration  $g =s_{l_{\zeta}}$. Whn the initial and goal configurations are implied, we refer to a path as simply $\zeta$.

Because there could be several paths to the goal, path planners typially employ a \emph{cost functional}, $C(\zeta)$ often defined as the sum of the costs between two subsequent configurations in a path $c(s_i,s_j)$. The total cost $C(\zeta)$ is therefore the sum of the individual costs along the path, i.e.,
\begin{equation}
	C(\zeta) = \sum_{i=0}^{l_{\zeta}-1} c(s_i,s_{i+1}).
\end{equation}
This cost functional is similar to the one encountered in optimal control as well as to value functions used in reinforcement learning. Given the cost functional, the path planner seeks an optimal path $\zeta^*$, which satisfies,
\begin{equation}
 	\zeta^*_{o,g} = \argmin_{\zeta_{o,g} \in Z_{o,g}} C(\zeta), \label{eq:back_plan}
\end{equation}
where $Z_{o,g}$ is the set of all possible paths such that $s_1 = o, s_{l_\zeta} = g$.

Many path planning algorithms discretise $\mathcal{S}$ and use graph search algorithms like $A^*$ to find the optimal path. Under mild assumptions, these approaches are guaranteed to find the best path on the graph, but not in $\mathcal{S}$. However, they scale poorly in the size of $\mathcal{S}$, as larger and larger graphs are required. Furthermore, such methods do not address kinodynamic constraints, i.e., they assume that every configuration on the graph is accessible from a neighbouring one. 

These drawbacks motivate \emph{sample-based} path planning algorithms such as RRT$^*$. Instead of building a graph and then searching it, RRT$^*$ builds a tree on the fly and keeps track of the current best path. As a result, this tree can be grown in a way that respects kinodynamic constraints. Furthermore, a sampled-based approach can quickly find good, but not optimal, paths in large configuration spaces, while avoiding obstacles.

%  RRT$^*$ is also asymptotically optimal, i.e., it is guranteed to find an optimal path in $\mathcal{S}$ as the sampling time goes to infinity. Many variants of RRT$^*$ provide improved performance and faster convergence in practice by, for example, heuristically shriking the sampling space \cite{gammell2014informed}.

% The most significant difference between A$^*$ and RRT$^*$ is that while former is deterministic, the latter is probabilistic. It is important to understand how this distinction affects learning.

In terms of the notation we have introduced, A$^*$ is capable of achieving the minimisation of \eqref{eq:back_plan} for a set $Z_{o,g}$ that is determined by its resolution (discretisation). RRT$^*$ on the other hand, for a given time budget T is capable of finding the maximum of a subset $\tilde{Z}_{o,g} \in  Z_{o,g}$ but in this case $Z_{o,g}$ is continouous and not restricted by any type of discretisation. Furthermore as $T \rightarrow \infty$ the RRT$^*$ performs a minimisation over $Z_{o,g}$, i.e., it is asymptotically optimal. Intuitively, the RRT$^*$ is asymptotically optimal as a function of sampling time and A$^*$ is asymptotically optimal as a function of resolution. This distintion will become very important when we attempt to learn cost functions for these planners in the next section.

\subsection{Inverse Reinforcement Learning for Path Planning \label{subsec:inverse_problem}}
Path planning involves finding a (near) optimal path to the goal given a cost function. In the inverse problem, we are given example paths and must find the cost function for which these paths are (near) optimal.  The example paths comprise a dataset $\mathcal{D} = (\zeta^1_{o_1,g_1},\zeta^2_{o_2,g_2}...\zeta^D_{o_D,g_D})$ where $\zeta^i_{o_i,g_i}$ is an example path with initial and final configurations $o_i,g_i$ respectively. We assume the unknown cost function is of the form,
\begin{equation}
	c(s_i,s_j) = \mathbf{w}^T \mathbf{f}(s_i,s_j), \label{eq:inner_prod}
\end{equation}
where $\mathbf{f}(s_i,s_j)$ is a $K$-dimensional vector of features that encode different aspects of the configuration pair and $\mathbf{w}$ is a vector of unknown weights to be learned. Since $\mathbf{w}$ is independent of the configuration, we can express the total cost of the path in a parametric form:

\begin{equation}
	C(\zeta) = \mathbf{w}^T\sum_{i=0}^{l_{\zeta}-1} \mathbf{f}(s_i,s_{i+1}) := \mathbf{w}^T \mathbf{F}(\zeta),
\end{equation}
where $\mathbf{F}(\zeta)$ is the \emph{feature sum} of the path.

While many formulations of the inverse problem exist, the general idea is to find a weight vector that assigns less cost to the example paths than all other possible paths with the same initial and goal configuration.  This can be formalised by a set of ineuality constraints:

\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq  C(\zeta) \quad \forall \zeta \in Z_{o_i,g_i}  \quad \forall i. \label{eq:const1}
\end{equation}

The sets $Z_{o_i,g_i}$ can be very large, however if we have access to an optimisation procedure like the one in in Equation \eqref{eq:back_plan} it is enough to satisfy, 

\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq \min_{\zeta_{o_i,g_i} \in Z_{o_i,g_i}} C(\zeta) \quad \forall i, \label{eq:const}
\end{equation}
since all other paths are guaranteed to have a larger cost than the minimum.
The constraint is an inequality because $Z_{o,g}$ contains only paths available to the planner and thus may not include the example path $\zeta^i_{o_i,g_i}$.

Ratliff et al.\ \cite{ratliff2006maximum} propose a maximum margin variant of \eqref{eq:const} by introducing a margin function $L_i(\zeta)$ that decreases the cost of the proposed path $\zeta$ if is dissimilar to the example path $\zeta^i_{o_i,g_i}$. For example, $L_i(\zeta)$ could be $-1$ times the number of configurations in the demonstration path not visited by $\zeta$. The full optimisation formulation of Maximum Margin Planning is as follows.

\begin{equation}
	\argmin_{\mathbf{w},\tau} \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \tau_i \label{eq:mas_marg}
\end{equation}
\begin{equation}
	\text{s.t.} \quad C(\zeta^i_{o_i,g_i}) - \tau_i \leq \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta) \quad \forall i,
\end{equation}
where $\tau_i$ are slacks that can be used to relax the constraints. Rearranging the inequality in terms of the slacks we get:

\begin{equation}
	 \quad C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta)  \leq \tau_i  \quad \forall i,
\end{equation}
meaning that minimising,
\begin{equation}
	\argmin_{\mathbf{w}} \big[ \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \big( C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}}\big(C(\zeta) + L_i(\zeta)\big) \big) \big]. \label{eq:unconstrained}
\end{equation}
is equivalent to minimizing \eqref{eq:mas_marg}, i.e., the slacks are tight.
The minimum can be found by computing a subgradient and performing gradient descent on the above objective:
\begin{equation}
	\nabla_{\mathbf{w}} =\mathbf{w} +  \frac{\gamma}{D} \sum_{i=0}^D F(\zeta^i_{o_i,g_i}) - F(\tilde{\zeta}^*_{o_i,g_i}), \label{eq:update1}
\end{equation}
where,
\begin{equation}
	\tilde{\zeta}^*_{o_i,g_i} = \argmin_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta). \label{eq:augmented_max}
\end{equation}

The inverse problem can therefore be seen as an iterative procedure, that first solves \eqref{eq:augmented_max} in the inner loop while keeping the weights constant. Based on that solution, it updates the weights using \eqref{eq:update1} in the outer loop. The weights at convergence represent the cost function that is used to plan the future behaviour of the agent. In \cite{ratliff2006maximum}, A$^*$ search was used for planning in the inner loop, assuming that the domain contained acyclic positive costs. In this paper, we make the same assumptions but develop methods that use RRT$^*$ for planning.

\section{Method}
	In this section we describe how feature sums are calculated for sampled based planners. We then go on to modify the maximum margin formulation to one that is appropriate for sample based planners. We explain the implications of this modifications and offer some insight as to their interpretation. Finally we propose an sampling based learning algorithm that leverages the nature of the RRT$^*$ to speed up the learning procedure.
	\subsection{Feature Sums and Sampled Based Planners}
		Feature sums, $F(\zeta)$, can be seen as the  `fingerprints' of paths, and their definition is of crucial importance in the inverse problem. When planning on a fixed graph, like in the case of MDPs or A$^*$ search, it is easy to define the feature sums as the \emph{edge} costs on the graph. Furthermore if planning in continous domains we may consider integrals along the path. In RRT's however the tree that is being built has neither a fixed structure, nor an analytical form. Thus, the first step towards learning RRT$^*$ cost functions from demonstration is to come up with a reasonable approximation of $F(\zeta)$ for a path. This approximation must be consistent with the definition of the cost for a configuration pair. One such definition for cost is as follows.
		\begin{equation}
			c(s_i,s_j) = \frac{c(s_i)+c(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Where $c(s_i)$ is a cost function defined over a single configuration and $d_{s_i,s_j}$ is a measure of distance between the two configurations. For example, if we consider configurations to be points in Eucledian space then $d_{s_i,s_j}$ can be the eucledian distance. Noting that the costs and features are related by \eqref{eq:inner_prod} for a single feature $f_k$ we have,

		\begin{equation}
			f_k(s_i,s_j) = \frac{f(s_i)+f(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Following this definition feature sum calculation along a candidate path is trivial,
		\begin{equation}
			F(\zeta) = \sum_{i=0}^{\zeta_l-1} \frac{\mathbf{f}(s_i)+\mathbf{f}(s_{i+1})}{2}d_{s_i,s_{i+1}}
		\end{equation}



	\subsection{Approximate Maximum Margin Planning \label{subsec:ammp}}

		In Section \ref{subsec:inverse_problem}  we reasoned about how the infinite constraints of Equation \eqref{eq:const1} could be reduced to a single constraint for each example resulting in Equation \eqref{eq:const}. In section \ref{subsec:path_planning} however we discussed that the RRT$^*$ can only achieve the minimisation over the full set of paths $Z_{o,g}$ if given infinite time.
		This ofcourse is not feasible. Therefore, in our formulation we will assume that for a certain time budget T, the RRT$^*$ performs a minimisation over for an unknown set $\tilde{Z}_{o,g}$ and hence satisfies a modified version of \eqref{eq:const},
\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq \min_{\zeta_{o_i,g_i} \in \tilde{Z}_{o_i,g_i}} C(\zeta) \quad \forall i. \label{eq:const_rrt}
\end{equation}
	And as a consequence we can rewrite the unconstrained objective function of Equation \eqref{eq:unconstrained} as:

	\begin{equation}
	\argmin_{\mathbf{w}} \big[ \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \big( C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in \tilde{Z}_{o_i,g_i}}\big(C(\zeta) + L_i(\zeta)\big) \big) \big]. \label{eq:unconstrained_rrt}
	\end{equation}
	This gives rise to a similar learning procedure to before with the crucial difference that the planning step is executed by an RRT$*$ planner and not a deterministic one, like A$^*$. Because of this distinction, while before the set $Z_{o_i,g_i}$ was fixed for all iterations of the learning, now $\tilde{Z}_{o_i,g_i}$ changes every time we invoke the sample based planner. As a result we can interpret the sample-based planning procedure of RRT$*$ as a smart way of sampling \emph{constraints} that we want our cost function to satisfy.

	It is also important to note that $\tilde{Z}_{o_i,g_i}$ not only changes for every learning iteration, but it is also unknown. Furthermore the dependance of $\tilde{Z}_{o_i,g_i}$ on the time budget T is very hard to quantify since it also depends on the size and nature of $S$ as well as the cost function we are using -which also changes with every iteration. For this reason, in this paper we resort to an experimental assesment of the ability of RRT$^*$ to sample the right constraints at every iteration and hence allow the learning of a cost function from a set of demonstrated paths.

\subsection{Cached RRT$^*$ for faster learning \label{subsec:cached}}
	In Section \ref{subsec:inverse_problem} we explained that learning cost function for path planning algorithms from demonstration is an iterative procedure where a planner is invoked for a certain configuration of the weights, for every set of initial conditions found in the dataset. Based on the resulting plans the weights are updated in a direction that would bring the resulting paths closer to the ones in the demonstrations. For a prodecure involving $I$ iterations and a dataset of size $|D|$, the planner must be invoked $I\times|D|$ times. Depending on the size of $S$ planning can be a costly procedure, and might render learning impractical. In this section we propose a method that can speed up the learning by exploiting the nature of the RRT$^*$ planner.

	Two of the most costly operations of the RRT$^*$ argorithm are, 1). Finding the nearest neighbour to a newly sampled point and 2). Finding the radius-neighbours of a newly created vertex in the tree \cite{karaman2011sampling}. However these procedures of the algorithm are completely independent of the cost function that the planner is using. This means that the RRT$^*$ algorithm can be split in two separate independent steps. 

	The first step is described in Algorithm \ref{alg:rrt_cache}. Input to this procedure how many points to randomly sample from the free space, $p$, the initial point $s_init$ and the steer step size $\eta$. For each of the randomly sampled points $s_{rand}$ we find the nearest neighbour, $s_{nearest}$, from the set of points in the vertex set V. We then create a new configuration point $s_{new}$ by steering from $s_{nearest}$ to $s_{rand}$. Next, we query the radius neighbours of $s_{new}$ at a radius determined by  $\min\{\gamma_{RRT^*}(\frac{\log(|V|)}{|V|})^{\frac{1}{d}},\eta\}$. Here, $d$ is the dimentionality of $S$, and $\gamma_{RRT^*}$ is??. The points $s_{new}$,$s_{nerest}$ and the set $S_{near}$ are stored together in the map \texttt{pointCache}, which is returned at the end of the procedure. Using this procedure we effectively turn the sampling process of the RRT$^*$ algorithm into a preprocessing step. This means that the expensive \texttt{Nearest} and \texttt{Near} procedures involved, only need to be repeated $|D|$ times instead of $I\times|D|$ times.




	\begin{algorithm}
	\caption{\texttt{cacheRRT}($p$,$s_{init}$,$\eta$)}
	\label{alg:rrt_cache}
	\begin{algorithmic}[1]
	\STATE $pointCache \gets \emptyset$
	\STATE $V \gets {s_{init}}$
	\FOR{$i=0 \dots p $}
	\STATE $s_{rand} \gets SampleFree_i$
	\STATE $s_{nearest} \gets \texttt{Nearest}(V,s_{rand})$
	\STATE $s_{new} \gets \texttt{Steer}(s_{nearest},s_{rand})$
	\STATE $S_{near} \gets \texttt{Near}(V,{s_{new}},\min\{\gamma_{RRT^*}(\frac{\log(|V|)}{|V|})^{\frac{1}{d}},\eta\})$ 
	\STATE $V\gets V \cup s_{new}$
	\STATE $pointCache \gets pointCache \cup \{s_{new},S_{near}\}$
	\ENDFOR
	\RETURN $pointCache$

	\end{algorithmic}
	\end{algorithm}

	The result of Algorithm \ref{alg:rrt_cache} can now act as input to the second procedure described in Algorithm \ref{alg:plan_cached}. The procedure is very similar to the RRT$^*$ wiring and re-wiring procedures in \cite{karaman2011sampling} and returns a minimum cost path to the goal. An important difference however is that the verteces of the tree and their neighbours at each iteration are already known. This speeds up the computation while keeping the planner used during learning and the planner used to execute the final cost function identical. As learning proceeds and the cost function we are required to plan with changes, so will the wiring of this tree, but the points involved in this procedure will not. 


	\begin{algorithm}
	\caption{\texttt{planCachedRRT$^*$}($pointCache$,$s_{init}$,$c()$)}
	 \label{alg:plan_cached}
	\begin{algorithmic}[1]
	\STATE $E \gets \emptyset$
	\STATE $V \gets {s_{init}}$
	\FOR{$i=0 \dots |pointCache| $}
	\STATE $s_{nearest} \gets pointCache\{s_{nearest}^i\}$
	\STATE $s_{new} \gets pointCache\{s_{new}^i\}$
	\STATE $S_{near} \gets pointCache\{S_{near}^i\}$
	\STATE $V\gets V \cup s_{new}$
	\STATE $s_{min}\gets s_{nearest}$
	\STATE $c_{min}\gets \texttt{Cost}(s_{nearest}) + c(s_{nearest},s_{new})$
	\FOR{$s_{near} \in S_{near} $}
	\STATE $c_{near} \gets \texttt{Cost}(s_{near}) + c(s_{near},s_{new})$
	\IF {\texttt{CollisionFree}($s_{near},s_{new}$) and  $c_{near}<c_{new}$}
	\STATE $s_{min} \gets s_{near}; c_{min}\gets c_{near}$
	\ENDIF
	\ENDFOR
	\STATE $E \gets E \cup \{(s_{min},s_{new})\} $
	\FOR{$s_{near} \in S_{near} $}
	\STATE $c_{new} \gets \texttt{Cost}(s_{near}) + c(s_{near},s_{new})$
	\IF {\texttt{CollisionFree}($s_{near},s_{new}$) and  $c_{new}<\texttt{Cost}(s_{near})$}
	\STATE $s_{parent} \gets \texttt{Parent}(s_{near})$
	\STATE $E \gets E  \smallsetminus {(s_{parent},s_{near})} \cup {(s_{new},s_{near})} $
	\ENDIF
	\ENDFOR
	\ENDFOR
	\STATE $\zeta_{min} \gets \texttt{minCostPath}(V,E,c())$
	\RETURN $\zeta_{min}$

	\end{algorithmic}
	\end{algorithm}

	The full aproximate Maximum Margin Planning procedure to learn RRT$^*$ cost functions from demonstration is described in Algorithm \ref{alg:ammp}. First we initialise the weights, either ramdomly or using a cost function that simply favours shortest paths. Then, for each datapoint $\zeta_i$ we calculate feature sums and run \texttt{cacheRRT}. The main learning loop involves cycling through all datapoints and finding the best path under a loss augmented cost function. The feature sums of this path are calculated and subsequently the difference with the demonstrated feature sums is computed. At the end of each iteration an average gradient is calculated and the cost function is updated. At convergence the learned weights are returned.

	\begin{algorithm}
	\caption{\texttt{AproximateMMP}($D,p,\eta,\lambda,\delta$)\label{alg:ammp}}
	 \label{alg:plan_cached}
	\begin{algorithmic}[1]
	\STATE $\mathbf{w} \gets \texttt{initialiseWeights}$
	\STATE $\mathbf{\tilde{F}} \gets \emptyset$
	\STATE $R \gets \emptyset$
	\FOR{$\zeta^i \text{ in } D$}
	\STATE $\tilde{F}_{\zeta^i} \gets \texttt{FeatureSums}(\zeta^i)$
	\STATE $\mathbf{\tilde{F}} \gets \mathbf{\tilde{F}} \cup \tilde{F}_{\zeta^i}$
	\STATE $r_i \gets \texttt{cacheRRT}(p,s_{init}^{\zeta^i},\eta)$
	\STATE $R \gets R \cup r_i $
	\ENDFOR
	\REPEAT
	\STATE $\nabla_{\mathbf{w}}\gets 0$
	\FOR{$ \zeta^i \text{in } D $}
	\STATE $c() \gets \texttt{getCostmap}(\mathbf{w}) + L(\zeta^i)$ 
	\STATE $r_i \gets R\{i\}$ ;	$\tilde{F}_i \gets \mathbf{\tilde{F}}\{i\}$ 
	\STATE $\zeta \gets \texttt{planCachedRRT}^*(r_i,x^i_{init},c())$
	\STATE $F_i \gets \texttt{FeatureSums}(\zeta)$
	\STATE $\nabla_{\mathbf{w}} \gets \nabla_{\mathbf{w}} + \tilde{F}_i - F_i $
	\ENDFOR
	\STATE $\nabla_{\mathbf{w}} \gets \lambda\mathbf{w} + \nabla_{\mathbf{w}} / |D|$
	\STATE $\mathbf{w} \gets \mathbf{w} - \delta\nabla_{\mathbf{w}} $
	\UNTIL{convergence}
	\RETURN $V,E$

	\end{algorithmic}
	\end{algorithm}

	% \STATE $\widetilde{\mu}^{\mathcal{D}} \gets \mathtt{empiricalFE}(\mathcal{D})$\hfill \COMMENT{using \eqref{eqn:empirical_fe}}
	% \STATE $\widetilde{\mu}^{\mathcal{F}} \gets \mathtt{empiricalFE}(\mathcal{F})$ 
	% \STATE $P_{\mathcal{D}}^{s_1} \gets \mathtt{initialStateDistribution}(\mathcal{D})$
	% \STATE $P_{\mathcal{F}}^{s_1} \gets \mathtt{initialStateDistribution}(\mathcal{F})$
	% \STATE $w^{\mathcal{F}}_k\gets 0\quad\forall k\in\{1,\ldots,K\}$
	% \REPEAT
	% \STATE $R(s,a) \gets (w^{\mathcal{D}}+w^{\mathcal{F}})^T\phi(s,a)\quad\forall s\in\mathcal{S},a\in\mathcal{A}$
	% \STATE $\pi \gets \mathtt{softPlan}(\mathcal{S},\mathcal{A},T,R)$\hfill\COMMENT{using \eqref{eq:soft_backup}}
	% \STATE $\mu^\pi|_{\mathcal{D}} = \mathtt{calculateFE}(\pi,T,P_{\mathcal{D}}^{s_1})$
	% \STATE $\mu^\pi|_{\mathcal{F}} = \mathtt{calculateFE}(\pi,T,P_{\mathcal{F}}^{s_1})$
	% \STATE $w^{\mathcal{D}} \leftarrow w^{\mathcal{D}} - \alpha (\mu^\pi|_{\mathcal{D}} - \widetilde{\mu}^{\mathcal{D}})$
	% \STATE $w^{\mathcal{F}} \leftarrow \frac{(\mu^\pi|_{\mathcal{F}} - \widetilde{\mu}^{\mathcal{F}})}{\lambda}$

	% \IF {$\lambda > \lambda_{min}$}
	% \STATE $\lambda \leftarrow \alpha_{\lambda}\lambda$
	% \ENDIF
	% \UNTIL{convergence}
	% \RETURN $R,\pi$





\section{Experiments}
	In Section \ref{subsec:ammp} we discussed how the original MMP algorithm can be applied to RRT$^*$ and in Section \ref{subsec:cached} we introdused a cached version of RRT$^*$ that allows faster learning. In this section we compare the performance of these two proposed algorithms against the original MMP algorithm, using A$^*$ search.
	
	Our experiments take place in the context of socially appropriate navigation. IRL has been widely used in this setting \cite{okallearning}, \cite{henry2010learning},\cite{vasquez2014inverse}, because it is usually very hard to hardcode the cost functions that a planner should use in complex social situations. Having the ability to quickly and effectively learn social navigation cost functions from demonstration would be a major asset for robots that operate in crowded environments such as airports, museums, care centers and shopping malls (need citations). 
	\subsection{Setting}
	Our experiments take place in a randomly generated social environment, shown in Figure \ref{fig:exp_setting}. Every arrow in the figure represents a person's position and orientation. The robot is given the task of navigating from one point in the room to the other. While it is aware of the orientation and position of different people, it has no idea on how to prioritise reaching the target quickly, avoiding people and nearing obstacles, i.e., the cost function is unknown. Instead, the robot is given a number of demonstrations in the form of a dataset $D$. Each demonstration $\zeta_i$ comes as a set of $(x,y)$ positions of the robot in the configuration space and each demonstration takes place for a different random configuration of the social environment, i.e., the people are at different positions and orientations every time. The task of the robot is to, using $D$, extract a cost function based on different features of the environment, which in turn would allow it to behave socially in future tasks.

	The features involved in our experiments include three different gaussian functions of different means and diagonal covariances around each person, three different functions that measure distance from the goal and a feature that measures proximity to obstacles. An example cost function over the whole configuration space is shown in Figure \ref{fig:cost_f} 

	\begin{figure}
	\centering
	%\vspace{-3.3mm}
	    \includegraphics[width=0.30\textwidth]{images/people.png}
	%\vspace{-4mm}
	  \caption{An instance of the randomised social navigation task used in the experiments. Arrows denote the position and orientation of people in the scene. The robot can be seen in the bottom right hand corner and the green box denotes the goal location.} \label{fig:exp_setting}
	\end{figure}

	\begin{figure}
	\centering
	%\vspace{-3.3mm}
	    \includegraphics[width=0.30\textwidth]{images/cost_f.png}
	%\vspace{-4mm}
	  \caption{An instance of the cost function used to collect ground truthed data for our experiments. Red denotes \emph{low} cost, while purple denotes \emph{high} cost} \label{fig:cost_f}
	\end{figure}
	\subsection{Evaluation}

	To evaluate the effectiveness of our algorithms we resort to both objective as well as subjective comparisons.

	To objectively assess the quality of our algorithms we generate a dataset $D^{obj}$ by planning near-optimal paths from an initial configuration $s_o$ to a goal configuration $s_g$ under a ground truth cost function $c_{gt}()$ derived from respective ground truth weights $\mathbf{w}_{gt}$. As mentioned in section \ref{subsec:path_planning} a fully optimal path can only be derived only assymptotically in terms of either time in terms of the RRT$^*$, or resolution in the case of A$^*$. In practice however we found that planning for 60s using the RRT$^*$ algorithm achieves a path that is very close to optimal. This is concluded by the change in optimal path cost achieved if more sampling time is given to the algorithm. This ground truth dataset is extremmely useful for evaluation. For each path $\zeta$ generated by the learner we know its respective cost under the ground truth reward function simply through the inner product $\mathbf{w}_{gt}$ with the paths feature sums $\mathbf{F}(\zeta)$. Furthermore we can compute the cost difference between the generated path and the example path by,
	\begin{equation}
		Q(\zeta,\zeta_i,\mathbf{w}) = \mathbf{w}(F(\zeta)-F(\zeta_i)). \label{eq:obj_eval}
	\end{equation} 
	Assuming the demonstration path $\zeta_i$ is optimal under $\mathbf{w}$, means $Q(\zeta,\zeta_i,\mathbf{w})>=0$, although for the reasons mentioned at the begining of this section it is very hard to guarantee this. Equation \ref{eq:obj_eval} will be our main means of evaluating the performance of different algorithms.

	Since one of our main contributions in this paper claims a speedup in learning time, we will also use the time per learning iteration as a means of objective evaluation. Although the speed of both RRT$^*$ and A$^*$ may very significantly across implementations, certain algorithmic characteristics do not, for example a higher grid resolution for A$^*$ will always result in increased planning time. For both algorithms we use vanilla implementations in Python. In fact all algorithms share similar functions and do not benefit from any kind of special optimisations appart from the one described in Section \ref{subsec:cached}.


	If the dataset $D$ comes from human demonstrations, where the underlying cost function is unknown, then it is much harder to objectively evaluate performance. For this reason we will simply resort to visual comparisons of the demonstrated and generated paths.

	\subsection{Results}

	Our objective dataset $D^{obj}$ consists of 20 trajectories at random social situations within the configuration space shown in Figure \ref{fig:exp_setting}, using the cost function shown in Figure \ref{fig:cost_f}. Half of these trajectories make up the training dataset $D^{obj}_{train}$ and the other half the test dataset $D^{obj}_{train}$. After being trained on $D^{obj}_{train}$ the performance of a cost function is evaluated on $D^{obj}_{test}$ using \ref{eq:obj_eval}. We report both the mean and median across the different trajectories in the test and training sets, for every iteration in Figures \ref{fig:train_results} and \ref{fig:train_results}. Results are reported for four different planners. Results marked as RRT$^*$ refer to aproximate MMP using an RRT$^*$ planner with a planning time of 12s. note that this is a significantly lower planning time than the one used to generate the near optimal demonstration paths in $D^{obj}$. Results marked as Cached RRT$*$ refer to the algorithm proposed in Section \ref{subsec:cached} for $p=2500$, which is the amount of points that would typically be sampled in 12s of planning time. Finally we report results for A$^*$ planners of two different grid resolutions namely 0.8 and 0.3 meters.

	Our first observation is that both methods that use RRT$^*$ as a planner perform better in all situations. This can be atributed to the fact that RRT$^*$ is not confined to work on a fixed grid allowing it to generate paths that are actually closer to optimal. We also observe that although the high resolution A$^*$ planner performs well in training it performs worse in testing. It is also worth noting that the aparent bad performance of the A$^*$ planners in Figure \ref{fig:train_cd_mean} can be atributed to bad performance with respect to one specific path. In this case it is perhaps more enlightening to look at the median values for performance.

	Table \ref{tab:time} the average planning time per learning iteration and the total learning time for all algorithms. Comparing between leaening usign a normal RRT$^*$ and a cached RRT$^*$ we can see that the latter is much faster and seems to trade off none of the performance. Furthermore both algorithms are faster than A$^*$ on a fine grid, but much slower than A$^*$ on a coarse grid. Another important point is the large variance in planning time when in comes to A$^*$ at 0.3m grid resolution. At the start of learning the initial cost function is very simple and and only involves the distance from the goal location. Under this cost function planning is very quick, because a simple heuristic that simple takes into account the distance from the goal is a very good aproximation to the cost-to-go from a certain configuration to the end. As learning proceeds however and the cost function becomes more complex, this simple heuristic, although admisible, no longer faithfuly represents the cost-to-go. This requires the A$^*$ planner to explore much more states before an optimal path is found. We can see therefore that A$^*$ does not only face scale problems as the size of $S$ increases but also as the cost function becomes more complex. Luckily, the probabilistic nature of the RRT$^*$ makes it less suceptible to these pathologies.  

\begin{figure}[tbh]
	\centering
%	\hspace{-5cm}
      \hspace{-5mm}
      \begin{subfigure}[b]{0.455\columnwidth}

    \includegraphics[clip=true,width=1.25\textwidth]{images/cost_diff_med.png}
    \caption{Median cost difference}
    \label{fig:train_cd_med}
  \end{subfigure}
 % \hspace{5mm}
 \hspace{5mm}
  \begin{subfigure}[b]{0.455\columnwidth}
    \includegraphics[clip=true,width=1.25\textwidth]{images/cost_diff.png}
    \caption{Mean cost difference}
    \label{fig:train_cd_mean}
  \end{subfigure} 

  %\vspace{-3mm}
  \caption{Mean and median cost difference on the training set, for 15 iterations}
  %\vspace{-3mm}
  \label{fig:train_results}

\end{figure}


\begin{figure}[tbh]
	\centering
%	\hspace{-5cm}
      \hspace{-5mm}
      \begin{subfigure}[b]{0.455\columnwidth}

    \includegraphics[clip=true,width=1.25\textwidth]{images/cost_diff_med_test.png}
    \caption{Median cost difference}
    \label{fig:test_cd_med}
  \end{subfigure}
 % \hspace{5mm}
 \hspace{5mm}
  \begin{subfigure}[b]{0.455\columnwidth}
    \includegraphics[clip=true,width=1.25\textwidth]{images/cost_diff_test.png}
    \caption{Mean cost difference}
    \label{fig:test_cd_mean}
  \end{subfigure} 

  %\vspace{-3mm}
  \caption{Mean and median cost difference on the test set, for 15 iterations}
  %\vspace{-3mm}
  \label{fig:test_results}

\end{figure}

	\begin{table}[]
	\centering
	\label{tab:time}
	\begin{tabular}{|l|l|l|l|l|}
	\hline
	                        & A* 0.8m    & A* 0.3m      & RRT*  & Cache RRT* \\ \hline
	Iteration (s) & 1.83(0.79) & 20.93(12.14) & 12(0) & 5.87(0.50)  \\ \hline
	Learning (s) & 275.2      & 3140.6       & 1808  & 911.7       \\ \hline
	\end{tabular}
	\caption{Per iteration and total learning times for our proposed algorithms and the baselines}
	\end{table}


\bibliographystyle{IEEEtran}
\bibliography{references}



\end{document}
