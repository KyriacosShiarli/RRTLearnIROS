%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass{article}  % Comment this line out if you need a4paper
\usepackage{ijcai16}
\usepackage{times}
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Rapidly Exploring Learning Trees
}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\DeclareMathOperator*{\argmin}{\arg\!\min} 
\DeclareMathOperator*{\argmax}{\arg\!\max} 
\author{Kyriacos Shiarlis \\
University of Amsterdam \\
k.c.shiarlis@uva.nl
\And 
Joao Messias \\
University of Amsterdam \\ 
jmessias@uva.nl 
\And
Shimon Whiteson \\
University of Oxford \\
shimon.whiteson@cs.ox.ac.uk 
}
\newcommand{\jm}[1]{\textcolor{blue}{Joao: #1}}

\newcommand{\sw}[1]{\textcolor{red}{SW: #1}}
\newcommand{\ks}[1]{\textcolor{green}{SW: #1}}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Learning from demonstration (LfD) \cite{argall2009survey} is of great interest to roboticists because it can avoid the need for tedious manual programming of complex behaviours. While most LfD methods rely on supervised learning to directly learn policies, certain approaches, namely inverse optimal control (IOC) \cite{kalman1964linear} and inverse reinforcement learning (IRL) \cite{abbeel2004apprenticeship} instead learn \emph{cost functions} from demonstration. These cost functions are then used to \emph{plan} the robot's behaviour. 

Cost functions tend to be more general and robust to changes in the environment. For example, if the friction in a manipulator's joints changes. A robot trained in a supervised manner has less chance of adapting than one that has been trained using IRL or IOC because the cost function remains unchanged. In addition, cost functions are thought to be more succint representations of the aims of the agent \cite{abbeel2004apprenticeship}. %For example, an agent that whose aim is to get as fast as possible to a goal location. The policy for such an agent will be much harder to interpret when compared to its cost function. 

However, inverse methods also have practical limitations. IRL for example requires modeling the task as a Markov decision process (MDP), which is typically impractical in robotics. for several reasons  Firstly, an MDP assumes full observability. Secondly, planning in an MDP, which IRL methods must do repeatedly, is costly, especially in continuous and high-dimensional domains. Finally, the lack of scalability and the assumption of full observability prohibit the natural modeling of important factors such as kinodynamic constraints. 

For these reasons, several researchers have proposed replacing the planning step in IRL with more convienient planners, e.g., Maximum Margin Planning (MMP) \cite{ratliff2006maximum} uses $A^*$ search for planning. This avoids the need to do all the planning in advance, as is typical when solving an MDP. It also avoids the need to model all aspects of the environment since the robot can quickly replan its trajectory to account for uncertainty. However, deterministic search methods such as A$^*$ require discretisation of the state space, and quickly run into scalability problems as the discretisation becomes finer. Furthermore, this discretisation again ignores kinodynamic constraints.

In path planning, Rapidly Exploring Random Trees (RRTs) \cite{lavalle1998rapidly} are popular because they cope well with continuous an high-dimensional domains and can elegantly handle kinodynamic constraints. The RRT$^*$ algorithm \cite{karaman2011sampling}, which extends RRTs to incorporate a cost function, is especially useful. However, the cost functions used by RRT$^*$ are typically simple and hand-coded.  Furthermore, to our knowledge, no methods have been developed to learn RRT$^*$ cost functions from demonstrations.

In this paper, we present methods for inverse reinforcement learning of RRT$^*$ cost functions. Our methods require no additional planner assumptions other than those inherent in RRT$^*$, making them particularly easy to implement. We begin by showing that the sample-based nature of RRT$^*$ requires a novel formulation of the learning problem. We then formulate algorithms that allow these cost functions to be learned effectively. Finally, we evaluate our methods on real and simulated data from a social navigation scenario. 

\section{Related Work}

\sw{General IRL related work here.}

Substantial research has applied IRL to robotics \cite{henry2010learning},\cite{abbeel2008apprenticeship}, \cite{vasquez2014inverse}. Because of the need to model the environment as an MDP, researchers usually discretise the state-action space. This introduces several key limitations. First, the size of the state-action space encountered in robotics is often prohibitive for such models. Second, the specific kinodynamic constraints of the robot cannot be easily be expressed by such representations, as they introduce non-Markovian dynamics. \ks{Depending on the state-action space discretisation, past actions can infuence how easy it is to move to any a subsequent state. However the transition function only considers only one action in the past, and it is therefore an expectation over possible past configurations. As a result it tends to give the wrong probability distribution over next states for the actual physical system. Do I have to explain this in detail? Maybe Joao has a citation from his research?}

To apply IRL to more realistic situations, researchers usually try to replace the MDP model while retaining the main idea behind IRL, i.e., learning the underlying cost function of a planner using data from demonstrations. 
 In \cite{ziebart2010modelingthesis} for example the Maximum Entropy IRL method was shown to work in domains of linear continouous dynamics with the optimal controller being a Linear-Quadratic Regulator. Since linear dynamics are hard to come by in robotics, in \cite{2012-cioc} locally linear aproximations are considered. These bring about locally consistent rewards, and achieve good performance in a range of tasks that were previously too hard for MDPs. However the optimisation of the cost function using such planners can never be guaranteed to be optimal. As our method is based on an RRT$^*$ planner it comes with the guarantee of asymptotic optimality.

Other approaches to the problem use hybrid planners. Inverse Optimal Heuristic Control (IOHC) \cite{ratliff2009inverse} models the long term goals of the agent as a coarse state MDP, to ensure tractability, while using supervised learning to determine the local policy of the agent at each state. Graph-Based IOC \cite{byravan2015graph} uses discrete optimal control on a coarse graph and the actual path is executed using local trajectory optimisation techinques such as CHOMP \cite{ratliff2009chomp}, which would otherwise suffer from local minima. The method is shown to be effective in robotic manipulation where the number of degrees of freedom would otherwise pose problems for MDPs. While effective, these methods employ complex and domain specific planning formulations whose implementation might be undesirable for other robotics tasks. The method presented here employs widely used planners, making them versatile and easy to implement. 

Another more recent graph-based concept is that of adaptive state graphs used in \cite{okallearning}. In this work the authors build a controller graph before doing any learning. This controller graph is akin to \emph{options} in semi-Markov decision processes, and allows for a more flexible representation of the state-action space. However, the controller used to learn underlying cost function is not the same as the one used to execute the robot's behaviour. 

Our work, can also be viewed as a graph, or rather tree-based approach. Instead of building a controller graph first and then using different controllers to optimise trajectories, we build a controller \emph{tree} on the fly.


\section{Background}
We begin with background on path planning and inverse reinforcement learning for path planning.

\subsection{Path Planning}
Path planning occurs in a space $\mathcal{S}$ of possible configurations of the robot. A configuration $s \in \mathcal{S}$ is usually continuous, and often represents spatial quantities such as position and orientation. A path planner seeks an obstacle-free path $\zeta_{o,g} = (s_1,s_2,s_3$ $\ldots,s_{l_{\zeta}}) $ of length $l_{\zeta}$, from an initial configuration $o = s_1$ to a goal configuration  $g =s_{l_{\zeta}}$. Whn the initial and goal configurations are implied, we refer to a path as simply $\zeta$.

Because there could be several paths to the goal, path planners typially employ a \emph{cost functional}, $C(\zeta)$ often defined as the sum of the costs between two subsequent configurations in a path $c(s_i,s_j)$. The total cost $C(\zeta)$ is therefore the sum of the individual costs along the path, i.e.,
\begin{equation}
	C(\zeta) = \sum_{i=0}^{l_{\zeta}-1} c(s_i,s_{i+1}).
\end{equation}
This cost functional is similar to the one encountered in optimal control as well as to value functions used in reinforcement learning. Given the cost functional, the path planner seeks an optimal path $\zeta^*$, which satisfies,
\begin{equation}
 	\zeta^*_{o,g} = \argmin_{\zeta_{o,g} \in Z_{o,g}} C(\zeta), \label{eq:back_plan}
\end{equation}
where $Z_{o,g}$ is the set of all possible paths such that $s_1 = o, s_{l_\zeta} = g$.

Many path planning algorithms discretise $\mathcal{S}$ and use graph search algorithms like $A^*$ to find the optimal path. Under mild assumptions, these approaches are guaranteed to find the best path on the graph, but not in $\mathcal{S}$. However, they scale poorly in the size of $\mathcal{S}$, as larger and larger graphs are required. Furthermore, such methods do not address kinodynamic constraints, i.e., they assume that every configuration on the graph is accessible from a neighbouring one. 

These drawbacks motivate \emph{sample-based} path planning algorithms such as RRT$^*$. Instead of building a graph and then searching it, RRT$^*$ builds a tree on the fly and keeps track of the current best path. As a result, this tree can be grown in a way that respects kinodynamic constraints. Furthermore, a sampled-based approach can quickly find good, but not optimal, paths in large configuration spaces, while avoiding obstacles.RRT$^*$ is also asymptotically optimal, i.e., it is guranteed to find an optimal path in $\mathcal{S}$ as the sampling time goes to infinity. Many variants of RRT$^*$ provide improved performance and faster convergence in practice by, for example, heuristically shriking the sampling space \cite{gammell2014informed}.

\subsection{Inverse Reinforcement Learning for Path Planning}
Path planning involves finding a (near) optimal path to the goal given a cost function. In the inverse problem, we are given example paths and must find the cost function for which these paths are (near) optimal.  The example paths comprise a dataset $\mathcal{D} = (\zeta^1_{o_1,g_1},\zeta^2_{o_2,g_2}...\zeta^D_{o_D,g_D})$ where $\zeta^i_{o_i,g_i}$ is an example path with initial and final configurations $o_i,g_i$ respectively. We assume the unknown cost function is of the form,
\begin{equation}
	c(s_i,s_j) = \mathbf{w}^T \mathbf{f}(s_i,s_j), \label{eq:inner_prod}
\end{equation}
where $\mathbf{f}(s_i,s_j)$ is a $K$-dimensional vector of features that encode different aspects of the configuration pair and $\mathbf{w}$ is a vector of unknown weights to be learned. Since $\mathbf{w}$ is independent of the configuration, we can express the total cost of the path in a parametric form:

\begin{equation}
	C(\zeta) = \mathbf{w}^T\sum_{i=0}^{l_{\zeta}-1} \mathbf{f}(s_i,s_{i+1}) := \mathbf{w}^T \mathbf{F}(\zeta),
\end{equation}
where $\mathbf{F}(\zeta)$ is the \emph{feature sum} of the path.

While many formulations of the inverse problem exist, the general idea is to find a weight vector that assigns less cost to the example paths than all other possible paths with the same initial and goal configuration.  This can be formalised by a set of ineuality constraints:

\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq  C(\zeta) \quad \forall \zeta \in Z_{o_i,g_i}  \quad \forall i. \label{eq:const1}
\end{equation}

The sets $Z_{o_i,g_i}$ can be very large, however if we have access to an optimisation procedure like the one in in Equation \eqref{eq:back_plan} it is enough to satisfy, 

\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq \min_{\zeta_{o_i,g_i} \in Z_{o_i,g_i}} C(\zeta) \quad \forall i, \label{eq:const}
\end{equation}
since all other paths are guaranteed to have a larger cost than the minimum.
The constraint is an inequality because $Z_{o,g}$ contains only paths available to the planner and thus may not include the example path $\zeta^i_{o_i,g_i}$.

Ratliff et al.\ \cite{ratliff2006maximum} propose a maximum margin variant of \eqref{eq:const} by introducing a margin function $L_i(\zeta)$ that decreases the cost of the proposed path $\zeta$ if is dissimilar to the example path $\zeta^i_{o_i,g_i}$. For example, $L_i(\zeta)$ could be $-1$ times the number of configurations in the demonstration path not visited by $\zeta$. The full optimisation formulation of Maximum Margin Planning is as follows.

\begin{equation}
	\argmin_{\mathbf{w},\tau} \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \tau_i \label{eq:max_marg}
\end{equation}
\begin{equation}
	\text{s.t.} \quad C(\zeta^i_{o_i,g_i}) - \tau_i \leq \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta) \quad \forall i,
\end{equation}
where $\tau_i$ are slacks that can be used to relax the constraints. Rearranging the inequality in terms of the slacks we get:

\begin{equation}
	 \quad C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta)  \leq \tau_i  \quad \forall i,
\end{equation}
meaning that minimising,
\begin{equation}
	\argmin_{\mathbf{w}} \big[ \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \big( C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}}\big(C(\zeta) + L_i(\zeta)\big) \big) \big]. \label{eq:unconstrained}
\end{equation}
is equivalent to minimizing \eqref{eq:max_marg}, i.e., the slacks are tight.
The minimum can be found by computing a subgradient and performing gradient descent on the above objective:
\begin{equation}
	\nabla_{\mathbf{w}} =\mathbf{w} +  \frac{\gamma}{D} \sum_{i=0}^D F(\zeta^i_{o_i,g_i}) - F(\tilde{\zeta}^*_{o_i,g_i}), \label{eq:update1}
\end{equation}
where,
\begin{equation}
	\tilde{\zeta}^*_{o_i,g_i} = \argmin_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta). \label{eq:augmented_max}
\end{equation}

The inverse problem can therefore be seen as an iterative procedure, that first solves \eqref{eq:augmented_max} in the inner loop while keeping the weights constant. Based on that solution, it updates the weights using \eqref{eq:update1} in the outer loop. The weights at convergence represent the cost function that is used to plan the future behaviour of the agent. In \cite{ratliff2006maximum}, A$^*$ search was used for planning in the inner loop, assuming that the domain contained acyclic positive costs. In this paper, we make the same assumptions but develop methods that use RRT$^*$ for planning.

\section{Methods}
	\subsection{Feature Sums and Sampled Based Planners}
		Feature sums, $F(\zeta)$, can be seen as the  `fingerprints' of paths, and their definition is of crucial importance in the inverse problem. When planning on a fixed graph, like in the case of MDPs or A$^*$ search, it is easy to define the feature sums as the \emph{edge} costs on the graph. Furthermore if planning in continous domains we may consider integrals along the path. In RRT's however the tree that is being built has neither a fixed structure, nor an analytical form. Thus, the first step towards learning RRT$^*$ cost functions from demonstration is to come up with a reasonable approximation of $F(\zeta)$ for a path. This approximation must be consistent with the definition of the cost for a configuration pair. One such definition for cost is as follows.
		\begin{equation}
			c(s_i,s_j) = \frac{c(s_i)+c(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Where $c(s_i)$ is a cost function defined over a single configuration and $d_{s_i,s_j}$ is a measure of distance between the two configurations. For example, if we consider configurations to be points in Eucledian space then $d_{s_i,s_j}$ can be the eucledian distance. Noting that the costs and features are related by \eqref{eq:inner_prod} for a single feature $f_k$ we have,

		\begin{equation}
			f_k(s_i,s_j) = \frac{f(s_i)+f(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Following this definition feature sum calculation along a candidate path is trivial,
		\begin{equation}
			F(\zeta) = \sum_{i=0}^{\zeta_l-1} \frac{\mathbf{f}(s_i)+\mathbf{f}(s_{i+1})}{2}d_{s_i,s_{i+1}}
		\end{equation}


	\subsection{Learning on a time budget}
		The most significant difference between A$^*$ and RRT$^*$ is that while former is deterministic, the latter is probabilistic. It is important to understand how this distinction affects learning.

		 In terms of the notation we have introduced, A$^*$ is capable of achieving the minimisation of \eqref{eq:back_plan} for a set $Z_{o,g}$ that is determined by its resolution (discretisation). RRT$^*$ on the other hand, for a given time budget T is capable of finding the maximum of a subset $\tilde{Z}_{o,g} \in  Z_{o,g}$ but in this case $Z_{o,g}$ is continouous and not restricted by any type of discretisation. Furthermore as $T \rightarrow \infty$ the RRT$^*$ performs a minimisation over $Z_{o,g}$, i.e., it is asymptotically optimal. Intuitively, the RRT$^*$ is asymptotically optimal as a function of sampling time and A$^*$ is asymptotically optimal as a function of resolution. This has an important implication when minimising \eqref{eq:unconstrained}. Before the maximisation over $Z_{o,g}$ was defined by the discretisation and could be solved relatively quickly for coarse graphs. Now this maximisation is infeasible unless the planning time $T$ is infinite so we will need to resort of approximations. The optimality of RRT$^*$ as a function of $T$ is highly dependent on $\mathcal{S}$ and the cost function (i need a citation here), making it very difficult to know how good our approximations are. In this paper we present an empirical demonstration that learning RRT$^*$ cost functions from demonstration is effective. 

	\subsection{Approximate Maximum Margin Planning}
		In the previous sections we reasoned about how the infinite constraints of Equation \eqref{eq:const1} could be reduced to a single constraint for each example resulting in Equation \eqref{eq:const}. As we explained above however this is no longer the case. What exactly should I claim here? That empirically things work well? What kind of theoretical results would be not too ambitious to pursue?

		


	\section{Experiments}
		In this section we evaluate the performance of aproximate maximum margin planning with RRT$^*$ as the base planner. To do so we i





	\section{Future}
		\subsection{Status}
		\begin{itemize}
			\item RRT for x,y configurations done. Adding orientation should be trivial.
			\item New simulator almost done. I am using Gazebo which also simulates physics, but I have been havign a lot of problems with the local planner (DWA). Maybe these will be fixed if I add orientation to the planner. I am thinking of incorporating a different local planner such as elastic bands. It seems to be better of my purposes.
			\item I can begin to collect data and even to learning, but I am not yet able to execute that plan reliably in control space (v,w).
		\end{itemize}
		\subsection{Plan}
		\begin{itemize}
			\item Finish Simulator by the end of the March. 
			\item Run a first comparison of A* and RRT* using max margin planning and cost-correlation similarity, by mid April. This will be done for the static human obstacle case. I.e. we will not be executing the plan.
			\item Implement a new idea I have and have a full set of results for all methods to be used by the end of April. Make a new assesment in terms of motivation and baselines.
			\item May: AAMAS poster, AAMAS, deliverable. 
			\item June: Collect data on static scenario and the real robot. Perform learning. Put it back on the robot.
			\item July: Atempt the dynamic cases. Find the best way to to do this. Perhaps consider a semi dynamic case.
			\item August: Finalise results and do the writeup, which I will also be doing in parallel.
		\end{itemize}

		\subsection{Concerns}
		\begin{itemize}
			\item It could be very hard to incorporate the dynamic cases. I need to perfect the local planner which I dont have now. From what I have seen in related work, other researchers simply plan on a `snapshot' of a dynamic case and simply demonstrate that path. In general, evaluation for such work is a bit strange.
			\item I can find no comparison between RRT* and A*. So I wonder how I should compare them. 
			People dont tend to use A* as a baseline for RRT* speed or path quality.
			\item I would really like to try this method on perhaps another dataset. But I cant figure ou what.
		\end{itemize}

	\section{Reasongs for IROS Failure}
		\subsection{Absense of a proper baseline}
			For the proposed scenarios (dataset), my baseline which is basically the objective in equation \eqref{eq:back_minim} was performing just as good as the maximum margin method.

			\textbf{Solution 1:} Use A* as a baseline. This seems like a natural baseline since existing IRL methods can be applied trivially to A*. These is also a max margin formulation for it. A* provides optimal planning and I wouldn’t really expect it to do worse than RRT* that is sub optimal. If we can however show that our algorithms can perform close to A* and can therefore be used whenever A* is not capable of performing well, i.e when the problem is high dimensional, or when there is a time budget for finding a solution.\\

			\textbf{Solution 2:} Come up with a harder scenario and beat the predefined baseline i.e. Max Margin vs No margin.\\

			\textbf{Solution 3:} Come up with a harder scenario and beat the predefined baseline and add Cost Correlation Similarity method. Hoping that it will be better than all of them.\\

			\textbf{Solution 4:} Try to beat a continuous trajectory optimisation method from this paper: http://graphics.stanford.edu/projects/cioc/cioc.pdf. I think that this is a strong baseline. It would show the applicability of our method in high dimentional control domains, if it succeeds. It would require lots of work in matlab to code the RRT and incorporate it in the examples. It would also require a slightly different definition for the RRT.

		\subsection{The dataset was trivial}
			For the proposed dataset there were a lot of cost functions that would peform reasonably. There was not really a difference in their performance on the 2 evaluation measures (path cost on ground truth cost function and similarity.). This was true both of the dataset collected from human demonstrations and the one collected using RRTs from a ground truth cost function.

				\textbf{Solution 1:} A more complicated navigation task? Perhaps even increasing the size of the room will help.\\

				\textbf{Solution 2:} Find some kind of simulated manipulation environment. The paper I mentioned before has a simple manipulation coded in matlab. Manipulators pose very big problems for A$^*$ because of the dimentionality and the kinodynamic constraints and they also pose a problem for trajectory optimisation methods because they are highly non-convex.

% Sampling based path planning algortihms such as Probabilistic Roadmaps(PRM) \cite{kavraki1996probabilistic} and Rapidly-Exploring-Random Trees(RRT)\cite{lavalle1998rapidly} have seen increasing popularity in various robotic planning applications, due to their ability to handle high dimentional continouous configuration spaces reliably and in reasonable time. In contrast to other planning and search algortihms such as A$^*$ and $D^*$, these algortihms do not require the specification of an admisible heuristic while sacrificing optimality for speed and state space coverage. While many interesting variants of the RRT have been invented such as the transition based RRT (T-RRT) \cite{jaillet2008transition}, by far the most significant contribution is that of the RRT$^*$ algorithm \cite{karaman2011sampling} where the original algorithm is extended to incorporate cost functions, based upon which an optimal path is guaranteed given an infinite amount of samples. These cost functions in essence determine the plan and as a result the behaviour of the robot in different situations, rendering their definition extremely important for roboticists.
% At times, rather than defining (and tuning) a cost function for a robot (or any other controled system) it is easier to demonstrate the desired plan or behaviour, and use algorithms to uncover the cost function that will bring about this behaviour. Examples of such methods lie in the fields of Inverse optimal control (IOC) and Inverse Reinforcement Learning (IRL)\cite{abbeel2004apprenticeship}, which due to their aforementioned property have been subject to increasing attention as well as success. A arguement for learning cost functions,
% rather than direct mappings to actions in a supervised manner [cite survey] is generality. In particular
% because cost functions are defined prior to planning, the policy of the agent(or robot) will differ if the
% environment (dynamics etc) changes while a policy learned by supervision will need to be revised. 

% Despite the appeal for learning planner cost functions, to our knowledge, there has been no known work that allows the learning of cost functions for RRT* planners (and its many derivatives [cite informed rrt]). This is exactly the contribution of this paper. 

% One application where RRTs are widely used is mobile robot navigation. At the same time, as robots enter public spaces, the environments in which robots are called to navigate have become increasingly complex[] and unpredictable. This means that a mobile robot's navigation cost function needs to be tediously tuned, in order to keep up with the task. An algorithm that can learn such a cost function from demonstration, without the need to change the planner would be a powerful tool in the arms of a roboticist.

%  % An essential assumption of these methods is that the planner used in the learning step of the algorithm must be used when planning under the learned cost function, otherwise the observed behaviour will not be compatible with the observations.
% Complex mobile robotics tasks, such as navigation are typically not modelled as control problems because of their computational complexity. Furthermore, modelling path planning as a Markov Decision Process (MDP) is usually avoided also due to its computational cost, with the exception of the case where this MDP is deterministic, which yields the same open loop policy as A$^*$. Therefore, even if our world was deterministic we would still be limited by the drawacks of A$^*$ planning from which probabilistic methods dont.\\

% It should be apparent that an algortihm that uses RRT$^*$ to learn as well as plan, would be of great value to roboticists seeking to incorporate human demonstrations into the design of their robots.


\bibliographystyle{IEEEtran}
\bibliography{references}



\end{document}
