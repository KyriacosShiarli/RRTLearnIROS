%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass{article}  % Comment this line out if you need a4paper
\usepackage{ijcai16}
\usepackage{times}
\usepackage{algorithmic}
\usepackage{algorithm}
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Rapidly Exploring Learning Trees
}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{\arg\!\min} 
\DeclareMathOperator*{\argmax}{\arg\!\max} 
\author{Kyriacos Shiarlis \\
University of Amsterdam \\
k.c.shiarlis@uva.nl
\And 
Joao Messias \\
University of Amsterdam \\ 
jmessias@uva.nl 
\And
Shimon Whiteson \\
University of Oxford \\
shimon.whiteson@cs.ox.ac.uk 
}
\newcommand{\jm}[1]{\textcolor{blue}{Joao: #1}}

\newcommand{\sw}[1]{\textcolor{red}{SW: #1}}
\newcommand{\ks}[1]{\textcolor{green}{SW: #1}}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Learning from demonstration (LfD) \cite{argall2009survey} is of great interest to roboticists because it can avoid the need for tedious manual programming of complex behaviours. While most LfD methods rely on supervised learning to directly learn policies, certain approaches, namely inverse optimal control (IOC) \cite{kalman1964linear} and inverse reinforcement learning (IRL) \cite{abbeel2004apprenticeship} instead learn \emph{cost functions} from demonstration. These cost functions are then used to \emph{plan} the robot's behaviour. 

Cost functions tend to be more general and robust to changes in the environment. For example, if the friction in a manipulator's joints changes. A robot trained in a supervised manner has less chance of adapting than one that has been trained using IRL or IOC because the cost function remains unchanged. In addition, cost functions are thought to be more succint representations of the aims of the agent \cite{abbeel2004apprenticeship}. %For example, an agent that whose aim is to get as fast as possible to a goal location. The policy for such an agent will be much harder to interpret when compared to its cost function. 

However, inverse methods also have practical limitations. IRL for example requires modeling the task as a Markov decision process (MDP), which is typically impractical in robotics. for several reasons  Firstly, an MDP assumes full observability. Secondly, planning in an MDP, which IRL methods must do repeatedly, is costly, especially in continuous and high-dimensional domains. Finally, the lack of scalability and the assumption of full observability prohibit the natural modeling of important factors such as kinodynamic constraints. 

For these reasons, several researchers have proposed replacing the planning step in IRL with more convienient planners, e.g., Maximum Margin Planning (MMP) \cite{ratliff2006maximum} uses $A^*$ search for planning. This avoids the need to do all the planning in advance, as is typical when solving an MDP. It also avoids the need to model all aspects of the environment since the robot can quickly replan its trajectory to account for uncertainty. However, deterministic search methods such as A$^*$ require discretisation of the state space, and quickly run into scalability problems as the discretisation becomes finer. Furthermore, this discretisation again ignores kinodynamic constraints.

In path planning, Rapidly Exploring Random Trees (RRTs) \cite{lavalle1998rapidly} are popular because they cope well with continuous an high-dimensional domains and can elegantly handle kinodynamic constraints. The RRT$^*$ algorithm \cite{karaman2011sampling}, which extends RRTs to incorporate a cost function, is especially useful. However, the cost functions used by RRT$^*$ are typically simple and hand-coded.  Furthermore, to our knowledge, no methods have been developed to learn RRT$^*$ cost functions from demonstrations.

In this paper, we present methods for inverse reinforcement learning of RRT$^*$ cost functions. Our methods require no additional planner assumptions other than those inherent in RRT$^*$, making them particularly easy to implement. We begin by showing that the sample-based nature of RRT$^*$ requires a novel formulation of the learning problem. We then formulate algorithms that allow these cost functions to be learned effectively. Finally, we evaluate our methods on real and simulated data from a social navigation scenario. 

\section{Related Work}

\sw{General IRL related work here.}

Substantial research has applied IRL to robotics \cite{henry2010learning},\cite{abbeel2008apprenticeship}, \cite{vasquez2014inverse}. Because of the need to model the environment as an MDP, researchers usually discretise the state-action space. This introduces several key limitations. First, the size of the state-action space encountered in robotics is often prohibitive for such models. Second, the specific kinodynamic constraints of the robot cannot be easily be expressed by such representations, as they introduce non-Markovian dynamics. \ks{Depending on the state-action space discretisation, past actions can infuence how easy it is to move to any a subsequent state. However the transition function only considers only one action in the past, and it is therefore an expectation over possible past configurations. As a result it tends to give the wrong probability distribution over next states for the actual physical system. Do I have to explain this in detail? Maybe Joao has a citation from his research?}

To apply IRL to more realistic situations, researchers usually try to replace the MDP model while retaining the main idea behind IRL, i.e., learning the underlying cost function of a planner using data from demonstrations. 
 In \cite{ziebart2010modelingthesis} for example the Maximum Entropy IRL method was shown to work in domains of linear continouous dynamics with the optimal controller being a Linear-Quadratic Regulator. Since linear dynamics are hard to come by in robotics, in \cite{2012-cioc} locally linear aproximations are considered. These bring about locally consistent rewards, and achieve good performance in a range of tasks that were previously too hard for MDPs. However the optimisation of the cost function using such planners can never be guaranteed to be optimal. As our method is based on an RRT$^*$ planner it comes with the guarantee of asymptotic optimality.

Other approaches to the problem use hybrid planners. Inverse Optimal Heuristic Control (IOHC) \cite{ratliff2009inverse} models the long term goals of the agent as a coarse state MDP, to ensure tractability, while using supervised learning to determine the local policy of the agent at each state. Graph-Based IOC \cite{byravan2015graph} uses discrete optimal control on a coarse graph and the actual path is executed using local trajectory optimisation techinques such as CHOMP \cite{ratliff2009chomp}, which would otherwise suffer from local minima. The method is shown to be effective in robotic manipulation where the number of degrees of freedom would otherwise pose problems for MDPs. While effective, these methods employ complex and domain specific planning formulations whose implementation might be undesirable for other robotics tasks. The method presented here employs widely used planners, making them versatile and easy to implement. 

Another more recent graph-based concept is that of adaptive state graphs used in \cite{okallearning}. In this work the authors build a controller graph before doing any learning. This controller graph is akin to \emph{options} in semi-Markov decision processes, and allows for a more flexible representation of the state-action space. However, the controller used to learn underlying cost function is not the same as the one used to execute the robot's behaviour. 

Our work, can also be viewed as a graph, or rather tree-based approach. Instead of building a controller graph first and then using different controllers to optimise trajectories, we build a controller \emph{tree} on the fly.


\section{Background}
We begin with background on path planning and inverse reinforcement learning for path planning.

\subsection{Path Planning \label{subsec:path_planning}}
Path planning occurs in a space $\mathcal{S}$ of possible configurations of the robot. A configuration $s \in \mathcal{S}$ is usually continuous, and often represents spatial quantities such as position and orientation. A path planner seeks an obstacle-free path $\zeta_{o,g} = (s_1,s_2,s_3$ $\ldots,s_{l_{\zeta}}) $ of length $l_{\zeta}$, from an initial configuration $o = s_1$ to a goal configuration  $g =s_{l_{\zeta}}$. Whn the initial and goal configurations are implied, we refer to a path as simply $\zeta$.

Because there could be several paths to the goal, path planners typially employ a \emph{cost functional}, $C(\zeta)$ often defined as the sum of the costs between two subsequent configurations in a path $c(s_i,s_j)$. The total cost $C(\zeta)$ is therefore the sum of the individual costs along the path, i.e.,
\begin{equation}
	C(\zeta) = \sum_{i=0}^{l_{\zeta}-1} c(s_i,s_{i+1}).
\end{equation}
This cost functional is similar to the one encountered in optimal control as well as to value functions used in reinforcement learning. Given the cost functional, the path planner seeks an optimal path $\zeta^*$, which satisfies,
\begin{equation}
 	\zeta^*_{o,g} = \argmin_{\zeta_{o,g} \in Z_{o,g}} C(\zeta), \label{eq:back_plan}
\end{equation}
where $Z_{o,g}$ is the set of all possible paths such that $s_1 = o, s_{l_\zeta} = g$.

Many path planning algorithms discretise $\mathcal{S}$ and use graph search algorithms like $A^*$ to find the optimal path. Under mild assumptions, these approaches are guaranteed to find the best path on the graph, but not in $\mathcal{S}$. However, they scale poorly in the size of $\mathcal{S}$, as larger and larger graphs are required. Furthermore, such methods do not address kinodynamic constraints, i.e., they assume that every configuration on the graph is accessible from a neighbouring one. 

These drawbacks motivate \emph{sample-based} path planning algorithms such as RRT$^*$. Instead of building a graph and then searching it, RRT$^*$ builds a tree on the fly and keeps track of the current best path. As a result, this tree can be grown in a way that respects kinodynamic constraints. Furthermore, a sampled-based approach can quickly find good, but not optimal, paths in large configuration spaces, while avoiding obstacles.

%  RRT$^*$ is also asymptotically optimal, i.e., it is guranteed to find an optimal path in $\mathcal{S}$ as the sampling time goes to infinity. Many variants of RRT$^*$ provide improved performance and faster convergence in practice by, for example, heuristically shriking the sampling space \cite{gammell2014informed}.

% The most significant difference between A$^*$ and RRT$^*$ is that while former is deterministic, the latter is probabilistic. It is important to understand how this distinction affects learning.

In terms of the notation we have introduced, A$^*$ is capable of achieving the minimisation of \eqref{eq:back_plan} for a set $Z_{o,g}$ that is determined by its resolution (discretisation). RRT$^*$ on the other hand, for a given time budget T is capable of finding the maximum of a subset $\tilde{Z}_{o,g} \in  Z_{o,g}$ but in this case $Z_{o,g}$ is continouous and not restricted by any type of discretisation. Furthermore as $T \rightarrow \infty$ the RRT$^*$ performs a minimisation over $Z_{o,g}$, i.e., it is asymptotically optimal. Intuitively, the RRT$^*$ is asymptotically optimal as a function of sampling time and A$^*$ is asymptotically optimal as a function of resolution. This distintion will become very important when we attempt to learn cost functions for these planners in the next section.

\subsection{Inverse Reinforcement Learning for Path Planning \label{subsec:inverse_problem}}
Path planning involves finding a (near) optimal path to the goal given a cost function. In the inverse problem, we are given example paths and must find the cost function for which these paths are (near) optimal.  The example paths comprise a dataset $\mathcal{D} = (\zeta^1_{o_1,g_1},\zeta^2_{o_2,g_2}...\zeta^D_{o_D,g_D})$ where $\zeta^i_{o_i,g_i}$ is an example path with initial and final configurations $o_i,g_i$ respectively. We assume the unknown cost function is of the form,
\begin{equation}
	c(s_i,s_j) = \mathbf{w}^T \mathbf{f}(s_i,s_j), \label{eq:inner_prod}
\end{equation}
where $\mathbf{f}(s_i,s_j)$ is a $K$-dimensional vector of features that encode different aspects of the configuration pair and $\mathbf{w}$ is a vector of unknown weights to be learned. Since $\mathbf{w}$ is independent of the configuration, we can express the total cost of the path in a parametric form:

\begin{equation}
	C(\zeta) = \mathbf{w}^T\sum_{i=0}^{l_{\zeta}-1} \mathbf{f}(s_i,s_{i+1}) := \mathbf{w}^T \mathbf{F}(\zeta),
\end{equation}
where $\mathbf{F}(\zeta)$ is the \emph{feature sum} of the path.

While many formulations of the inverse problem exist, the general idea is to find a weight vector that assigns less cost to the example paths than all other possible paths with the same initial and goal configuration.  This can be formalised by a set of ineuality constraints:

\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq  C(\zeta) \quad \forall \zeta \in Z_{o_i,g_i}  \quad \forall i. \label{eq:const1}
\end{equation}

The sets $Z_{o_i,g_i}$ can be very large, however if we have access to an optimisation procedure like the one in in Equation \eqref{eq:back_plan} it is enough to satisfy, 

\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq \min_{\zeta_{o_i,g_i} \in Z_{o_i,g_i}} C(\zeta) \quad \forall i, \label{eq:const}
\end{equation}
since all other paths are guaranteed to have a larger cost than the minimum.
The constraint is an inequality because $Z_{o,g}$ contains only paths available to the planner and thus may not include the example path $\zeta^i_{o_i,g_i}$.

Ratliff et al.\ \cite{ratliff2006maximum} propose a maximum margin variant of \eqref{eq:const} by introducing a margin function $L_i(\zeta)$ that decreases the cost of the proposed path $\zeta$ if is dissimilar to the example path $\zeta^i_{o_i,g_i}$. For example, $L_i(\zeta)$ could be $-1$ times the number of configurations in the demonstration path not visited by $\zeta$. The full optimisation formulation of Maximum Margin Planning is as follows.

\begin{equation}
	\argmin_{\mathbf{w},\tau} \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \tau_i \label{eq:max_marg}
\end{equation}
\begin{equation}
	\text{s.t.} \quad C(\zeta^i_{o_i,g_i}) - \tau_i \leq \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta) \quad \forall i,
\end{equation}
where $\tau_i$ are slacks that can be used to relax the constraints. Rearranging the inequality in terms of the slacks we get:

\begin{equation}
	 \quad C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta)  \leq \tau_i  \quad \forall i,
\end{equation}
meaning that minimising,
\begin{equation}
	\argmin_{\mathbf{w}} \big[ \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \big( C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}}\big(C(\zeta) + L_i(\zeta)\big) \big) \big]. \label{eq:unconstrained}
\end{equation}
is equivalent to minimizing \eqref{eq:max_marg}, i.e., the slacks are tight.
The minimum can be found by computing a subgradient and performing gradient descent on the above objective:
\begin{equation}
	\nabla_{\mathbf{w}} =\mathbf{w} +  \frac{\gamma}{D} \sum_{i=0}^D F(\zeta^i_{o_i,g_i}) - F(\tilde{\zeta}^*_{o_i,g_i}), \label{eq:update1}
\end{equation}
where,
\begin{equation}
	\tilde{\zeta}^*_{o_i,g_i} = \argmin_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta). \label{eq:augmented_max}
\end{equation}

The inverse problem can therefore be seen as an iterative procedure, that first solves \eqref{eq:augmented_max} in the inner loop while keeping the weights constant. Based on that solution, it updates the weights using \eqref{eq:update1} in the outer loop. The weights at convergence represent the cost function that is used to plan the future behaviour of the agent. In \cite{ratliff2006maximum}, A$^*$ search was used for planning in the inner loop, assuming that the domain contained acyclic positive costs. In this paper, we make the same assumptions but develop methods that use RRT$^*$ for planning.

\section{Method}
	In this section we describe how feature sums are calculated for sampled based planners. We then go on to modify the maximum margin formulation to one that is appropriate for sample based planners. We explain the implications of this modifications and offer some insight as to their interpretation. Finally we propose an sampling based learning algorithm that leverages the nature of the RRT$^*$ to speed up the learning procedure.
	\subsection{Feature Sums and Sampled Based Planners}
		Feature sums, $F(\zeta)$, can be seen as the  `fingerprints' of paths, and their definition is of crucial importance in the inverse problem. When planning on a fixed graph, like in the case of MDPs or A$^*$ search, it is easy to define the feature sums as the \emph{edge} costs on the graph. Furthermore if planning in continous domains we may consider integrals along the path. In RRT's however the tree that is being built has neither a fixed structure, nor an analytical form. Thus, the first step towards learning RRT$^*$ cost functions from demonstration is to come up with a reasonable approximation of $F(\zeta)$ for a path. This approximation must be consistent with the definition of the cost for a configuration pair. One such definition for cost is as follows.
		\begin{equation}
			c(s_i,s_j) = \frac{c(s_i)+c(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Where $c(s_i)$ is a cost function defined over a single configuration and $d_{s_i,s_j}$ is a measure of distance between the two configurations. For example, if we consider configurations to be points in Eucledian space then $d_{s_i,s_j}$ can be the eucledian distance. Noting that the costs and features are related by \eqref{eq:inner_prod} for a single feature $f_k$ we have,

		\begin{equation}
			f_k(s_i,s_j) = \frac{f(s_i)+f(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Following this definition feature sum calculation along a candidate path is trivial,
		\begin{equation}
			F(\zeta) = \sum_{i=0}^{\zeta_l-1} \frac{\mathbf{f}(s_i)+\mathbf{f}(s_{i+1})}{2}d_{s_i,s_{i+1}}
		\end{equation}



	\subsection{Approximate Maximum Margin Planning}

		In Section \ref{subsec:inverse_problem}  we reasoned about how the infinite constraints of Equation \eqref{eq:const1} could be reduced to a single constraint for each example resulting in Equation \eqref{eq:const}. In section \ref{subsec:path_planning} however we discussed that the RRT$^*$ can only achieve the minimisation over the full set of paths $Z_{o,g}$ if given infinite time.
		This ofcourse is not feasible. Therefore, in our formulation we will assume that for a certain time budget T, the RRT$^*$ performs a minimisation over for an unknown set $\tilde{Z}_{o,g}$ and hence satisfies a modified version of \eqref{eq:const},
\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq \min_{\zeta_{o_i,g_i} \in \tilde{Z}_{o_i,g_i}} C(\zeta) \quad \forall i. \label{eq:const_rrt}
\end{equation}
	And as a consequence we can rewrite the unconstrained objective function of Equation \eqref{eq:unconstrained} as:

	\begin{equation}
	\argmin_{\mathbf{w}} \big[ \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \big( C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in \tilde{Z}_{o_i,g_i}}\big(C(\zeta) + L_i(\zeta)\big) \big) \big]. \label{eq:unconstrained_rrt}
	\end{equation}
	This gives rise to a similar learning procedure to before with the crucial difference that the planning step is executed by an RRT$*$ planner and not a deterministic one, like A$^*$. Because of this distinction, while before the set $Z_{o_i,g_i}$ was fixed for all iterations of the learning, now $\tilde{Z}_{o_i,g_i}$ changes every time we invoke the sample based planner. As a result we can interpret the sample-based planning procedure of RRT$*$ as a smart way of sampling \emph{constraints} that we want our cost function to satisfy.

	It is also important to note that $\tilde{Z}_{o_i,g_i}$ not only changes for every learning iteration, but it is also unknown. Furthermore the dependance of $\tilde{Z}_{o_i,g_i}$ on the time budget T is very hard to quantify since it also depends on the size and nature of $S$ as well as the cost function we are using -and also changes with every iteration. For this reason, in this paper we resort to an experimental assesment of the ability of RRT$^*$ to sample the right constraints at every iteration and hence allow the learning of a cost function from a set of demonstrated paths.

\subsection{Cached RRT$^*$ for faster learning}
	In Section \ref{subsec:inverse_problem} we explain that learning cost function for path planning algorithms from demonstration is an iterative procedure where a planner is invoked for a certain configuration of the weights, for every set of initial conditions found in the dataset. Based on the resulting plans the weights are updated in such a direction that bring the resulting paths closer to the ones in the demonstrations. For a prodecure involving $I$ iterations and a dataset of size $|D|$, the planner must be invoked $I\times|D|$ times. Depending on the size of $S$ planning can be a costly procedure, which might render learning impractical. Here we propose a method, we call \emph{simulated sampling}, that can speed up the learning by exploiting the nature of the RRT$^*$ planner.

	Two of the most costly operations of the RRT$^*$ argorithm are, 1) Finding the nearest neighbour to a newly sampled point and 2) Finding the radius-neighbours of a newly created vertex in the tree \cite{karaman2011sampling}. However these procedures of the algorithm are completely independent of the cost function that the planner is using, and which changes at every iteration of learning. This means that the RRT$^*$ algorithm can be split in two separate independent steps. 

	The first step is described in Algorithm \ref{alg:rrt_cache}. Input to this procedure how many points to randomly sample from the free space, the initial point $x_init$ and the steer step size $\eta$. For each of the randomly sampled points $x_{rand}$ we find the nearest neighbour, $x_{nearest}$, from the set of points in the vertex set V. We then create a new configuration point $x_{new}$ by steering from $x_{nearest}$ to $x_{rand}$. Next, we query the radius neighbours of $x_{new}$ at a radius determined by  $\min\{\gamma_{RRT^*}(\frac{\log(|V|)}{|V|})^{\frac{1}{d}},\eta\}$. Here, $d$ is the dimentionality of $S$, and $\gamma_{RRT^*}$ is??. The point $x_{new}$ is and the set $X_{near}$ are stored together in the \texttt{pointCache}, which is returned at the end of the procedure. Using this procedure we effectively turn the sampling process of the RRT$^*$ algorithm into a preprocessing step. This means that the expensive \texttt{Nearest} and \texttt{Near} procedures involved, only need to be repeated $|D|$ times instead of $I\times|D|$ times.




	\begin{algorithm}
	\caption{\texttt{cacheRRT}($pointsToSample$,$x_{init}$,$\eta$)}
	\label{alg:rrt_cache}
	\begin{algorithmic}[1]
	\STATE $pointCache \gets \emptyset$
	\STATE $V \gets {x_{init}}$
	\FOR{$i=0 \dots pointsToSample $}
	\STATE $x_{rand} \gets SampleFree_i$
	\STATE $x_{nearest} \gets \texttt{Nearest}(V,x_{rand})$
	\STATE $x_{new} \gets \texttt{Steer}(x_{nearest},x_{rand})$
	\STATE $X_{near} \gets \texttt{Near}(V,{x_{new}},\min\{\gamma_{RRT^*}(\frac{\log(|V|)}{|V|})^{\frac{1}{d}},\eta\})$ 
	\STATE $V\gets V \cup x_{new}$
	\STATE $pointCache \gets pointCache \cup \{x_{new},X_{near}\}$
	\ENDFOR
	\RETURN $pointCache$

	\end{algorithmic}
	\end{algorithm}

	The result of Algorithm \ref{alg:rrt_cache} can now act as input to the second procedure described in Algorithm \ref{alg:plan_cached}. The procedure is identical to the RRT$^*$ wiring and re-wiring procedures in \cite{karaman2011sampling}, with the important difference that the vertexes of the tree and their neighbours at each iteration are already known. As learning proceeds and the cost function we are required to plan with changes, so will the wiring of this tree, but the points involved in this procedure are determined before. Furthermore, the wiring procedure of these points, is identical to that of RRT$^*$, this is crucial as we want to keep the planner used during learning and the planner used to execute the final cost function identical.


	\begin{algorithm}
	\caption{\texttt{planCachedRRT$^*$}($pointCache$,$x_{init}$)}
	\label{alg:rrt_cache}
	\begin{algorithmic}[1]
	\STATE $E \gets \emptyset$
	\STATE $V \gets {x_{init}}$
	\FOR{$i=0 \dots |pointCache| $}
	\STATE $x_{nearest} \gets pointCache\{x_{nearest}^i\}$
	\STATE $x_{new} \gets pointCache\{x_{new}^i\}$
	\STATE $X_{near} \gets pointCache\{X_{near}^i\}$
	\STATE $V\gets V \cup x_{new}$
	\STATE $x_{min}\gets x_{nearest}$
	\STATE $c_{min}\gets \texttt{Cost}(x_{nearest}) + c(x_{nearest},x_{new})$
	\FOR{$x_{near} \in X_{near} $}
	\STATE $c_{near} \gets \texttt{Cost}(x_{near}) + c(x_{near},x_{new})$
	\IF {\texttt{CollisionFree}($x_{near},x_{new}$) and  $c_{near}<c_{new}$}
	\STATE $x_{min} \gets x_{near}; c_{min}\gets c_{near}$
	\ENDIF
	\ENDFOR
	\STATE $E \gets E \cup \{(x_{min},x_{new})\} $
	\FOR{$x_{near} \in X_{near} $}
	\STATE $c_{new} \gets \texttt{Cost}(x_{near}) + c(x_{near},x_{new})$
	\IF {\texttt{CollisionFree}($x_{near},x_{new}$) and  $c_{new}<\texttt{Cost}(x_{near})$}
	\STATE $x_{parent} \gets \texttt{Parent}(x_{near})$
	\STATE $E \gets E  \smallsetminus {(x_{parent},x_{near})} \cup {(x_{new},x_{near})} $
	\ENDIF
	\ENDFOR
	\ENDFOR
	\RETURN $V,E$

	\end{algorithmic}
	\end{algorithm}


	% \STATE $\widetilde{\mu}^{\mathcal{D}} \gets \mathtt{empiricalFE}(\mathcal{D})$\hfill \COMMENT{using \eqref{eqn:empirical_fe}}
	% \STATE $\widetilde{\mu}^{\mathcal{F}} \gets \mathtt{empiricalFE}(\mathcal{F})$ 
	% \STATE $P_{\mathcal{D}}^{s_1} \gets \mathtt{initialStateDistribution}(\mathcal{D})$
	% \STATE $P_{\mathcal{F}}^{s_1} \gets \mathtt{initialStateDistribution}(\mathcal{F})$
	% \STATE $w^{\mathcal{F}}_k\gets 0\quad\forall k\in\{1,\ldots,K\}$
	% \REPEAT
	% \STATE $R(s,a) \gets (w^{\mathcal{D}}+w^{\mathcal{F}})^T\phi(s,a)\quad\forall s\in\mathcal{S},a\in\mathcal{A}$
	% \STATE $\pi \gets \mathtt{softPlan}(\mathcal{S},\mathcal{A},T,R)$\hfill\COMMENT{using \eqref{eq:soft_backup}}
	% \STATE $\mu^\pi|_{\mathcal{D}} = \mathtt{calculateFE}(\pi,T,P_{\mathcal{D}}^{s_1})$
	% \STATE $\mu^\pi|_{\mathcal{F}} = \mathtt{calculateFE}(\pi,T,P_{\mathcal{F}}^{s_1})$
	% \STATE $w^{\mathcal{D}} \leftarrow w^{\mathcal{D}} - \alpha (\mu^\pi|_{\mathcal{D}} - \widetilde{\mu}^{\mathcal{D}})$
	% \STATE $w^{\mathcal{F}} \leftarrow \frac{(\mu^\pi|_{\mathcal{F}} - \widetilde{\mu}^{\mathcal{F}})}{\lambda}$

	% \IF {$\lambda > \lambda_{min}$}
	% \STATE $\lambda \leftarrow \alpha_{\lambda}\lambda$
	% \ENDIF
	% \UNTIL{convergence}
	% \RETURN $R,\pi$





	\section{Experiments}
		In this section we evaluate the performance of aproximate maximum margin planning with RRT$^*$ as the base planner. To do so we i


\bibliographystyle{IEEEtran}
\bibliography{references}



\end{document}
