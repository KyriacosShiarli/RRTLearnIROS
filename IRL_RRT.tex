%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Rapidly Exploring Learning Trees
}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\DeclareMathOperator*{\argmin}{\arg\!\min} 
\DeclareMathOperator*{\argmax}{\arg\!\max} 
\author{Kyriacos Shiarlis, Joao Messias, and Shimon Whiteson% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
% \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%         University of Twente, 7500 AE Enschede, The Netherlands
%         {\tt\small albert.author@papercept.net}}%
% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small b.d.researcher@ieee.org}}%
}
\newcommand{\jm}[1]{\textcolor{blue}{Joao: #1}}

\newcommand{\sw}[1]{\textcolor{red}{SW: #1}}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\sw{Add author affiliations.}

Learning from demonstration (LfD) \cite{argall2009survey} is of great interest to roboticists because it can avoid the need for tedious manual programming of complex behaviours. While most LfD methods rely on supervised learning to directly learn policies, certain approaches, namely inverse optimal control (IOC) \cite{kalman1964linear} and inverse reinforcement learning (IRL) \cite{abbeel2004apprenticeship} instead learn \emph{cost functions} from demonstration. These cost functions are then used to \emph{plan} the robot's behaviour. 

Cost functions tend to be more general and robust to changes in the environment. For example, if the friction in a manipulator's joints changes. A robot trained in a supervised manner has less chance of adapting than one that has been trained using IRL or IOC because the cost function remains unchanged. In addition, cost functions are thought to be more succint representations of the aims of the agent \cite{abbeel2004apprenticeship}. %For example, an agent that whose aim is to get as fast as possible to a goal location. The policy for such an agent will be much harder to interpret when compared to its cost function. 

However, inverse methods also have practical limitations. IRL for example requires modeling the task as a Markov decision process (MDP), which is typically impractical in robotics. for several reasons  Firstly, an MDP assumes full observability. Secondly, planning in an MDP, which IRL methods must do repeatedly, is costly, especially in continuous and high-dimensional domains. Finally, the lack of scalability and the assumption of full observability prohibit the natural modeling of important factors such as kinodynamic constraints. 

For these reasons, several researchers have proposed replacing the planning step in IRL with more convienient planners, e.g., Maximum Margin Planning (MMP) \cite{ratliff2006maximum} uses $A^*$ search for planning. This avoids the need to do all the planning in advance, as is typical when solving an MDP. It also avoids the need to model all aspects of the environment since the robot can quickly replan its trajectory to account for uncertainty. However, deterministic search methods such as A$^*$ require discretisation of the state space, and quickly run into scalability problems as the discretisation becomes finer. Furthermore, this discretisation again ignores kinodynamic constraints.

In path planning, Rapidly Exploring Random Trees (RRTs) \cite{lavalle1998rapidly} are popular because they cope well with continuous an high-dimensional domains and can elegantly handle kinodynamic constraints. The RRT$^*$ algorithm \cite{karaman2011sampling}, which extends RRTs to incorporate a cost function, is especially useful. However, the cost functions used by RRT$^*$ are typically simple and hand-coded.  Furthermore, to our knowledge, no methods have been developed to learn RRT$^*$ cost functions from demonstrations.

In this paper, we present methods for inverse reinforcement learning of RRT$^*$ cost functions. Our methods require no additional planner assumptions other than those inherent in RRT$^*$, making them particularly easy to implement. We begin by showing that the sample-based nature of RRT$^*$ requires a novel formulation of the learning problem. We then formulate algorithms that allow these cost functions to be learned effectively. Finally, we evaluate our methods on real and simulated data from a social navigation scenario. 

\section{Related Work}

\sw{General IRL related work here.}

Substantial research has applied IRL to robotics \sw{cite}. Because of the need to model the environment as an MDP, researchers usually discretise the state-action space. This introduces several key limitations. First, the size of the state-action space encountered in robotics is often prohibitive for such models. Second, the specific kinodynamic constraints of the robot cannot be easily be expressed by such representations, as they introduce non-Markovian dynamics. \sw{why? this is still unclear.} 

In this paper we replace the typical planner used in IRL with one that is more convienient for robotic applications while retaining the main idea behind IRL, i.e., learning the underlying cost function to that planner using data from demonstrations. \sw{this paragraph seems out of place; it also makes our work sound very incremental.}

This challenge \sw{what challenge?} has been taken on several times by researchers that seek to make IRL more applicable to real world applications. In \cite{ziebart2010modelingthesis} for example the Maximum Entropy IRL method was shown to work in domains of linear continouous dynamics with the optimal controller being a Linear-Quadratic Regulator. Since linear dynamics are hard to come by in robotics, in \cite{2012-cioc} locally linear aproximations are considered. These bring about locally consistent rewards, and achieve good performance in a range of tasks that were previously too hard for MDPs. However the optimisation of the cost function using such planners can never be guaranteed to be optimal. \sw{Can ours?}

Other approaches to the problem use hybrid planners. Inverse Optimal Heuristic Control (IOHC) \cite{ratliff2009inverse} models the long term goals of the agent as a coarse state MDP, to ensure tractability, while using supervised learning to determine the local policy of the agent at each state. Graph-Based IOC \cite{byravan2015graph} uses discrete optimal control on a coarse graph and the actual path is executed using local trajectory optimisation techinques such as CHOMP \cite{ratliff2009chomp}, which would otherwise suffer from local minima. The method is shown to be effective in robotic manipulation where the number of degrees of freedom would otherwise pose problems for MDPs. While effective, these methods employ complex and domain specific planning formulations whose implementation might be undesirable for other robotics tasks. The method presented here employs widely used planners, making them versatile and easy to implement. 

Another more recent graph-based concept is that of adaptive state graphs used in \cite{okallearning}. In this work the authors build a controller graph before doing any learning. This controller graph is akin to \emph{options} in semi-Markov decision processes, and allows for a more flexible representation of the state-action space. However, the controller used to learn underlying cost function is not the same as the one used to execute the robot's behaviour. 

Our work, can also be viewed as a graph, or rather tree-based approach. Instead of building a controller graph first and then using different controllers to optimise trajectories, we build a controller \emph{tree} on the fly.


\section{Background}
We begin with background on path planning and inverse reinforcement learning for path planning.

\subsection{Path Planning}
Path planning occurs in a space $\mathcal{S}$ of possible configurations of the robot. A configuration $s \in \mathcal{S}$ is usually continuous, and often represents spatial quantities such as position and orientation. A path planner seeks an obstacle-free path $\zeta_{o,g} = (s_0,s_1,s_2$ $\ldots,s_{l_{\zeta}}) $ of length $l_{\zeta}$ \sw{isn't the length $l_{\zeta}+1$?}, from an initial configuration $o = s_0$ to a goal configuration  $g =s_{l_{\zeta}}$. Whn the initial and goal configurations are implied, we refer to a path as simply $\zeta$.

Because there could be several paths to the goal, path planners typially employ a \emph{cost functional}, $C(\zeta)$ often defined as the sum of the costs between two subsequent configurations in a path $c(s_i,s_j)$. The total cost $C(\zeta)$ is therefore the sum of the individual costs along the path, i.e.,
\begin{equation}
	C(\zeta) = \sum_{i=0}^{l_{\zeta}-1} c(s_i,s_{i+1}).
\end{equation}
This cost functional is similar to the one encountered in optimal control as well as to value functions used in reinforcement learning. Given the cost functional, the path planner seeks an optimal path $\zeta^*$, which satisfies,
\begin{equation}
 	\zeta^*_{o,g} = \argmin_{\zeta_{o,g} \in Z_{o,g}} C(\zeta), \label{eq:back_plan}
\end{equation}
where $Z_{o,g}$ is the set of all paths such that $s_0 = o, s_{l_\zeta} = g$.

Path planning is a well studied problem in robotics with many proposed solutions. One approach is to discretise $\mathcal{S}$ and use popular graph search algorithms, such as $A^*$ search to find the optimal path. Under very mild assumptions this approach is quarantied to find the best path on the graph -but not in $\mathcal{S}$-, but suffers from scalability problems as the size of $\mathcal{S}$ increases, e.g., as the discretisation becomes finer. Furthermore such methods do not take into account kinodynamic constraints that a robot might have, i.e., they assume that every configuration on the graph is acessible from a neighbouring one. The drawbacks of $A^*$ and its variants provide motivations for so called \emph{sample-based} path planning algorithms such as RRT$^*$. Instead of building a graph and then search it, RRT$^*$ builds a tree on the fly and keeps track of the current best path. As a result this tree can be grown in a way that respects the kinodynamic constraints of the robot. Furthermore its randomised nature allows it to find reasonable paths quickly in large configuration spaces, while avoiding obstacles. Furthermore RRT$^*$ is asymptotically optimal, i.e., an optimal path in the $\mathcal{S}$ is guarantied as the sampling time goes to infinity. Many variants of this algorithm provide improved performance and faster convergence in practice by, for example, heuristically shriking the sampling space \cite{gammell2014informed}.

\subsection{Inverse Reinforcement Learning for Path Planning}
The solution to the path planning problem above involves finding an optimal (or near optimal) path to the goal given a cost function. In this paper we are concerned with the inverse problem, i.e., given example paths, find the cost function that will result in these paths being optimal (or near optimal). These example paths come in the form of a dataset $\mathcal{D} = (\zeta^1_{o_1,g_1},\zeta^2_{o_2,g_2}...\zeta^D_{o_D,g_D})$ where $\zeta^i_{o_i,g_i}$ denotes an observed path with initial and final configurations $o_i,g_i$ respectively. Although the cost function that generated these observations is unknown, we assume that it is of the form,
\begin{equation}
	c(s_i,s_j) = \mathbf{w}^T \mathbf{f}(s_i,s_j). \label{eq:inner_prod}
\end{equation}
Where $\mathbf{f}(s_i,s_j)$ is a $K$ dimensional vector of fetures that encode different aspects of the configuration pair and $\mathbf{w}$ is a vector of unknown weights to be learned.
Using this notation, and since $\mathbf{w}$ is not configuration-dependant, we can express the total cost of the path in a parametric form,

\begin{equation}
	C(\zeta) = \mathbf{w}^T\sum_{i=0}^{l_{\zeta}-1} \mathbf{f}(s_i,s_{i+1}) := \mathbf{w}^T \mathbf{F}(\zeta),
\end{equation}
where $\mathbf{F}(\zeta)$ is the \emph{feature sum} of the path.

While many formulations of the inverse problem exist the general idea is that we seek a weight vector that assigns less cost to the example paths than all other possible paths at the same initial conditions and thus satisfy the constraints which is in turn equivalent to:

\begin{equation}
	C(\zeta^i_{o_i,g_i}) \leq \min_{\zeta_{o,g} \in Z_{o,g}} C(\zeta) \quad \forall i.
\end{equation}
Note that this constraint is in the form of an inequality because $Z_{o,g}$ only contains the set of paths available to the planner and may very well not include the actual example path $\zeta^i_{o_i,g_i}$.
In \cite{ratliff2006maximum} Ratliff et al. propose a maximum margin definition of the above constraint, by introducing a margin function $L_i(\zeta)$ that decreases the cost of the proposed path $\zeta$ if is not similar to the example path $\zeta^i_{o_i,g_i}$. An example of such a function is one that induces a -1 cost for all states not visited in the demonstration path and 0 otherwise.

 The full optimisation formulation of Maximum Margin Planning is as follows.

\begin{equation}
	\argmin_{\mathbf{w},\tau} \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \tau_i \label{eq:max_marg}
\end{equation}
\begin{equation}
	s.t \quad C(\zeta^i_{o_i,g_i}) - \tau_i \leq \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta) \quad \forall i,
\end{equation}
where $\tau_i$ are slacks that can be used to relax the constraints. Rearanging the inequality in terms of the slacks we get:

\begin{equation}
	 \quad C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta)  \leq \tau_i  \quad \forall i,
\end{equation}
meaning that minimising,
\begin{equation}
	\argmin_{\mathbf{w}} \frac{1}{2}||\mathbf{w}||^2 + \frac{\gamma}{D} \sum_i \big( C(\zeta^i_{o_i,g_i}) - \min_{\zeta \in Z_{o_i,g_i}}\big(C(\zeta) + L_i(\zeta)\big) \big).
\end{equation}
is equivalent to minimizing \eqref{eq:max_marg}, i.e., the slacks are tight.
The minimum can be found by computing a subgradient and performing gradient descent on the above objective:

\begin{equation}
	\nabla_{\mathbf{w}} =\mathbf{w} +  \frac{\gamma}{D} \sum_{i=0}^D F(\zeta^i_{o_i,g_i}) - F(\tilde{\zeta}^*_{o_i,g_i}). \label{eq:update1}
\end{equation}
Where,
\begin{equation}
	\tilde{\zeta}^*_{o_i,g_i} = \argmin_{\zeta \in Z_{o_i,g_i}} C(\zeta) + L_i(\zeta). \label{eq:augmented_max}
\end{equation}

The inverse problem can therefore be seen as an iterative procedure, that first solves Equation \eqref{eq:augmented_max} in the inner loop while keeping the weights constant. Based on that solution updates the weights using \eqref{eq:update1} in the outer loop. The weights at convergence represent the cost function that is used to plan the future behaviour of the agent. In \cite{ratliff2006maximum} A$^*$ search was used as a planning procedure for the inner loop assuming that the domain contained acyclic positive costs. In this paper we will make the same assumption and develop methods that use an RRT$^*$ as a base planner. 

\section{Methods}
	\subsection{Feature Sums and Sampled Based Planners}
		Feature sums, $F(\zeta)$, can be seen as the  `fingerprints' of paths, and their definition is of crucial importance in the inverse problem. When planning on a fixed graph, like in the case of MDPs or A$^*$ search, it is easy to define the feature sums as the \emph{edge} costs on the graph. Furthermore if planning in continous domains we may consider integrals along the path. In RRT's however the tree that is being built has neither a fixed structure, nor an analytical form. Thus, the first step towards learning RRT$^*$ cost functions from demonstration is to come up with a reasonable approximation of $F(\zeta)$ for a path. This approximation must be consistent with the definition of the cost for a configuration pair. One such definition for cost is as follows.
		\begin{equation}
			c(s_i,s_j) = \frac{c(s_i)+c(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Where $c(s_i)$ is a cost function defined over a single configuration and $d_{s_i,s_j}$ is a measure of distance between the two configurations. For example, if we consider configurations to be points in Eucledian space then $d_{s_i,s_j}$ can be the eucledian distance. Noting that the costs and features are related by \eqref{eq:inner_prod} for a single feature $f_k$ we have,

		\begin{equation}
			f_k(s_i,s_j) = \frac{f(s_i)+f(s_j)}{2}d_{s_i,s_j}
		\end{equation}
		Following this definition feature sum calculation along a candidate path is trivial,
		\begin{equation}
			F(\zeta) = \sum_{i=0}^{\zeta_l-1} \frac{\mathbf{f}(s_i)+\mathbf{f}(s_{i+1})}{2}d_{s_i,s_{i+1}}
		\end{equation}


	\subsection{Suboptimality of RRT$^*$}
		A major difference between RRT$^*$ and a deterministic planner such as A$^*$ lies in optimality. In terms of the notation we have introduced, A$^*$ is capable of achieving the minimisation of \eqref{eq:back_plan}. RRT$^*$ on the other hand, for a given time budget T is capable of sampling a subset $\tilde{Z}_{o,g} \in  Z_{o,g}$. The maximisation in \eqref{eq:back_plan} can then be performed over that subset. Furthermore this subset will contain $\zeta^*$ with probability 1 as $T \rightarrow \infty$, i.e the algorithm is asymptotically optimal \cite{karaman2011sampling}. Planning with this budget T is key to our contribution, precisely because our methods would be appropriate in cases where this budget is low when compared to the time required to solve the planning problem using traditional deterministic search algorithms. 

	\subsection{Approximate Maximum Margin Planning for RRT$^*$}
		Describe max margin here

	\subsection{Cost Correlation Similarity}
		Describe CSC here

	\section{Future}
		\subsection{Status}
		\begin{itemize}
			\item RRT for x,y configurations done. Adding orientation should be trivial.
			\item New simulator almost done. I am using Gazebo which also simulates physics, but I have been havign a lot of problems with the local planner (DWA). Maybe these will be fixed if I add orientation to the planner. I am thinking of incorporating a different local planner such as elastic bands. It seems to be better of my purposes.
			\item I can begin to collect data and even to learning, but I am not yet able to execute that plan reliably in control space (v,w).
		\end{itemize}
		\subsection{Plan}
		\begin{itemize}
			\item Finish Simulator by the end of the March. 
			\item Run a first comparison of A* and RRT* using max margin planning and cost-correlation similarity, by mid April. This will be done for the static human obstacle case. I.e. we will not be executing the plan.
			\item Implement a new idea I have and have a full set of results for all methods to be used by the end of April. Make a new assesment in terms of motivation and baselines.
			\item May: AAMAS poster, AAMAS, deliverable. 
			\item June: Collect data on static scenario and the real robot. Perform learning. Put it back on the robot.
			\item July: Atempt the dynamic cases. Find the best way to to do this. Perhaps consider a semi dynamic case.
			\item August: Finalise results and do the writeup, which I will also be doing in parallel.
		\end{itemize}

		\subsection{Concerns}
		\begin{itemize}
			\item It could be very hard to incorporate the dynamic cases. I need to perfect the local planner which I dont have now. From what I have seen in related work, other researchers simply plan on a `snapshot' of a dynamic case and simply demonstrate that path. In general, evaluation for such work is a bit strange.
			\item I can find no comparison between RRT* and A*. So I wonder how I should compare them. 
			People dont tend to use A* as a baseline for RRT* speed or path quality.
			\item I would really like to try this method on perhaps another dataset. But I cant figure ou what.
		\end{itemize}

	\section{Reasongs for IROS Failure}
		\subsection{Absense of a proper baseline}
			For the proposed scenarios (dataset), my baseline which is basically the objective in equation \eqref{eq:back_minim} was performing just as good as the maximum margin method.

			\textbf{Solution 1:} Use A* as a baseline. This seems like a natural baseline since existing IRL methods can be applied trivially to A*. These is also a max margin formulation for it. A* provides optimal planning and I wouldn’t really expect it to do worse than RRT* that is sub optimal. If we can however show that our algorithms can perform close to A* and can therefore be used whenever A* is not capable of performing well, i.e when the problem is high dimensional, or when there is a time budget for finding a solution.\\

			\textbf{Solution 2:} Come up with a harder scenario and beat the predefined baseline i.e. Max Margin vs No margin.\\

			\textbf{Solution 3:} Come up with a harder scenario and beat the predefined baseline and add Cost Correlation Similarity method. Hoping that it will be better than all of them.\\

			\textbf{Solution 4:} Try to beat a continuous trajectory optimisation method from this paper: http://graphics.stanford.edu/projects/cioc/cioc.pdf. I think that this is a strong baseline. It would show the applicability of our method in high dimentional control domains, if it succeeds. It would require lots of work in matlab to code the RRT and incorporate it in the examples. It would also require a slightly different definition for the RRT.

		\subsection{The dataset was trivial}
			For the proposed dataset there were a lot of cost functions that would peform reasonably. There was not really a difference in their performance on the 2 evaluation measures (path cost on ground truth cost function and similarity.). This was true both of the dataset collected from human demonstrations and the one collected using RRTs from a ground truth cost function.

				\textbf{Solution 1:} A more complicated navigation task? Perhaps even increasing the size of the room will help.\\

				\textbf{Solution 2:} Find some kind of simulated manipulation environment. The paper I mentioned before has a simple manipulation coded in matlab. Manipulators pose very big problems for A$^*$ because of the dimentionality and the kinodynamic constraints and they also pose a problem for trajectory optimisation methods because they are highly non-convex.

% Sampling based path planning algortihms such as Probabilistic Roadmaps(PRM) \cite{kavraki1996probabilistic} and Rapidly-Exploring-Random Trees(RRT)\cite{lavalle1998rapidly} have seen increasing popularity in various robotic planning applications, due to their ability to handle high dimentional continouous configuration spaces reliably and in reasonable time. In contrast to other planning and search algortihms such as A$^*$ and $D^*$, these algortihms do not require the specification of an admisible heuristic while sacrificing optimality for speed and state space coverage. While many interesting variants of the RRT have been invented such as the transition based RRT (T-RRT) \cite{jaillet2008transition}, by far the most significant contribution is that of the RRT$^*$ algorithm \cite{karaman2011sampling} where the original algorithm is extended to incorporate cost functions, based upon which an optimal path is guaranteed given an infinite amount of samples. These cost functions in essence determine the plan and as a result the behaviour of the robot in different situations, rendering their definition extremely important for roboticists.
% At times, rather than defining (and tuning) a cost function for a robot (or any other controled system) it is easier to demonstrate the desired plan or behaviour, and use algorithms to uncover the cost function that will bring about this behaviour. Examples of such methods lie in the fields of Inverse optimal control (IOC) and Inverse Reinforcement Learning (IRL)\cite{abbeel2004apprenticeship}, which due to their aforementioned property have been subject to increasing attention as well as success. A arguement for learning cost functions,
% rather than direct mappings to actions in a supervised manner [cite survey] is generality. In particular
% because cost functions are defined prior to planning, the policy of the agent(or robot) will differ if the
% environment (dynamics etc) changes while a policy learned by supervision will need to be revised. 

% Despite the appeal for learning planner cost functions, to our knowledge, there has been no known work that allows the learning of cost functions for RRT* planners (and its many derivatives [cite informed rrt]). This is exactly the contribution of this paper. 

% One application where RRTs are widely used is mobile robot navigation. At the same time, as robots enter public spaces, the environments in which robots are called to navigate have become increasingly complex[] and unpredictable. This means that a mobile robot's navigation cost function needs to be tediously tuned, in order to keep up with the task. An algorithm that can learn such a cost function from demonstration, without the need to change the planner would be a powerful tool in the arms of a roboticist.

%  % An essential assumption of these methods is that the planner used in the learning step of the algorithm must be used when planning under the learned cost function, otherwise the observed behaviour will not be compatible with the observations.
% Complex mobile robotics tasks, such as navigation are typically not modelled as control problems because of their computational complexity. Furthermore, modelling path planning as a Markov Decision Process (MDP) is usually avoided also due to its computational cost, with the exception of the case where this MDP is deterministic, which yields the same open loop policy as A$^*$. Therefore, even if our world was deterministic we would still be limited by the drawacks of A$^*$ planning from which probabilistic methods dont.\\

% It should be apparent that an algortihm that uses RRT$^*$ to learn as well as plan, would be of great value to roboticists seeking to incorporate human demonstrations into the design of their robots.


\bibliographystyle{IEEEtran}
\bibliography{references}



\end{document}
